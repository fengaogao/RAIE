import os, json
import argparse
from typing import List, Dict, Tuple
import numpy as np
from datetime import datetime
from collections import Counter

# ------------------------------
# 与你原脚本保持一致的工具函数
# ------------------------------

def format_item_token(mid: int) -> str:
    return f"<item_{mid}>"

def save_jsonl(path, rows):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def sliding_windows(seq: List[str], win_len: int, stride: int = 1, align: str = "end"):
    """
    右对齐滑窗：确保最后一次交互（seq[-1]）作为目标被包含。
    返回: (ctx, tgt, tgt_idx)
    """
    n = len(seq)
    if n < win_len:
        return
    if align == "end":
        start = (n - win_len) % stride
    else:  # "start"
        start = 0

    for i in range(start, n - win_len + 1, stride):
        window = seq[i:i + win_len]
        ctx, tgt = window[:-1], window[-1]
        yield ctx, tgt, (i + win_len - 1)

def build_examples_global_time_split(
    user2seq: Dict[int, List[Tuple[int, int]]],
    win_len: int = 10,
    stride: int = 1,
    q_orig: float = 0.5,
    q_test_start: float = 0.8
):
    ts_all = np.array([ts for pairs in user2seq.values() for _, ts in pairs], dtype=np.int64)
    assert ts_all.size > 0, "没有可用时间戳。"
    t0 = int(np.quantile(ts_all, q_orig))
    t1 = int(np.quantile(ts_all, q_test_start))
    if t1 <= t0: t1 = t0 + 1

    original, finetune, test = [], [], []
    # --- per-block stats containers ---
    users_o, items_o, inter_o = set(), set(), 0
    users_f, items_f, inter_f = set(), set(), 0
    users_t, items_t, inter_t = set(), set(), 0

    original_stride1 = []
    all_items = set()

    for uid, pairs in user2seq.items():
        mids = [m for m, _ in pairs]
        tss  = [ts for _, ts in pairs]

        mids_o = [m for (m, ts) in pairs if ts < t0]
        tss_o  = [ts for ts in tss if ts < t0]

        mids_f = [m for (m, ts) in pairs if t0 <= ts < t1]
        tss_f  = [ts for ts in tss if t0 <= ts < t1]

        mids_t = [m for (m, ts) in pairs if ts >= t1]
        tss_t  = [ts for ts in tss if ts >= t1]

        # --- accumulate raw positive interactions per block ---
        if len(mids_o) > 0:
            users_o.add(uid)
            items_o.update(mids_o)
            inter_o += len(mids_o)

        if len(mids_f) > 0:
            users_f.add(uid)
            items_f.update(mids_f)
            inter_f += len(mids_f)

        if len(mids_t) > 0:
            users_t.add(uid)
            items_t.update(mids_t)
            inter_t += len(mids_t)

        if len(mids_o) >= win_len:
            for (ctx, tgt, tgt_idx) in sliding_windows(mids_o, win_len, stride, align="end"):
                ts = tss_o[tgt_idx]
                prompt = " ".join(format_item_token(m) for m in ctx)
                target = format_item_token(tgt)
                original.append({
                    "user_id": uid, "timestamp": int(ts),
                    "prompt": prompt, "target": target,
                })
                all_items.update(ctx); all_items.add(tgt)

            for (ctx, tgt, tgt_idx) in sliding_windows(mids_o, win_len, 1, align="end"):
                ts = tss_o[tgt_idx]
                prompt = " ".join(format_item_token(m) for m in ctx)
                target = format_item_token(tgt)
                original_stride1.append({
                    "user_id": uid, "timestamp": int(ts),
                    "prompt": prompt, "target": target,
                })
                all_items.update(ctx); all_items.add(tgt)

        if len(mids_f) >= win_len:
            for (ctx, tgt, tgt_idx) in sliding_windows(mids_f, win_len, stride, align="end"):
                ts = tss_f[tgt_idx]
                prompt = " ".join(format_item_token(m) for m in ctx)
                target = format_item_token(tgt)
                finetune.append({
                    "user_id": uid, "timestamp": int(ts),
                    "prompt": prompt, "target": target,
                })
                all_items.update(ctx); all_items.add(tgt)

        if len(mids_t) >= win_len:
            for (ctx, tgt, tgt_idx) in sliding_windows(mids_t, win_len, stride, align="end"):
                ts = tss_t[tgt_idx]
                prompt = " ".join(format_item_token(m) for m in ctx)
                target = format_item_token(tgt)
                test.append({
                    "user_id": uid, "timestamp": int(ts),
                    "prompt": prompt, "target": target,
                })
                all_items.update(ctx); all_items.add(tgt)

    # --- pack stats ---
    stats = {
        "O": {"users": len(users_o), "items": len(items_o), "interactions": inter_o},
        "F": {"users": len(users_f), "items": len(items_f), "interactions": inter_f},
        "T": {"users": len(users_t), "items": len(items_t), "interactions": inter_t},
    }

    return {
        "original": original, "finetune": finetune, "test": test, "original_stride1": original_stride1,
        "all_items": sorted(all_items),
        "t0": t0, "t1": t1,
        "stats": stats,
    }

def save_stream(path, user2seq: Dict[int, List[Tuple[int,int]]], t0: int, t1: int, which: str):
    """
    which: 'original' | 'finetune'
    original: ts < t0
    finetune: t0 <= ts < t1
    """
    rows = []
    for uid, pairs in user2seq.items():
        if which == 'original':
            seg = [(m,ts) for m,ts in pairs if ts < t0]
        elif which == 'finetune':
            seg = [(m,ts) for m,ts in pairs if t0 <= ts < t1]
        else:
            raise ValueError("which must be 'original' or 'finetune'")
        if len(seg) < 2:
            continue
        mids = [m for m,_ in seg]
        tss  = [ts for _,ts in seg]
        rows.append({
            "user_id": uid,
            "items": " ".join(format_item_token(m) for m in mids),
            "timestamps": " ".join(str(ts) for ts in tss),
        })
    save_jsonl(path, rows)

# ------------------------------
# Yelp 专用：review.json -> user2seq
# ------------------------------

def _parse_date_to_ts(s: str) -> int:
    """
    Yelp review.json 的 date 通常是 'YYYY-MM-DD HH:MM:SS' 或 'YYYY-MM-DD'
    """
    s = s.strip()
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
        try:
            dt = datetime.strptime(s, fmt)
            return int(dt.timestamp())
        except ValueError:
            pass
    # 兜底：尝试 fromisoformat
    try:
        dt = datetime.fromisoformat(s)
        return int(dt.timestamp())
    except Exception:
        return 0  # 无法解析则返回0（后续会被清洗阶段过滤）

def load_yelp_reviews(review_path: str, min_stars: float = 4.0) -> Dict[int, List[Tuple[int, int]]]:
    """
    读取 Yelp Open Dataset 的 review.json（或 yelp_academic_dataset_review.json）
    输出：user2seq 字典： uid(int) -> [(business_id(int), timestamp), ...]（按时间升序）
    仅保留 stars >= min_stars 的交互，以匹配 MovieLens 中的阈值逻辑。
    """
    assert os.path.exists(review_path), f"review 文件不存在: {review_path}"

    user_id_map: Dict[str, int] = {}
    biz_id_map: Dict[str, int] = {}
    next_uid, next_mid = 1, 1
    user2seq: Dict[int, List[Tuple[int, int]]] = {}

    with open(review_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                continue

            # Yelp review.json 字段：user_id, business_id, stars, date, ...
            uid_s: str = obj.get("user_id", "")
            bid_s: str = obj.get("business_id", "")
            stars = obj.get("stars", None)
            date_s = obj.get("date", "")

            if uid_s == "" or bid_s == "" or stars is None:
                continue
            if float(stars) < float(min_stars):
                continue

            # map 到 int id（连续编号，便于后续构造 <item_xxx> 词表）
            if uid_s not in user_id_map:
                user_id_map[uid_s] = next_uid
                next_uid += 1
            if bid_s not in biz_id_map:
                biz_id_map[bid_s] = next_mid
                next_mid += 1

            uid = user_id_map[uid_s]
            mid = biz_id_map[bid_s]
            ts  = _parse_date_to_ts(date_s)
            user2seq.setdefault(uid, []).append((mid, ts))

    # 按时间升序
    for u in user2seq:
        user2seq[u].sort(key=lambda x: x[1])
    return user2seq

def _find_review_file(data_dir: str) -> str:
    """
    兼容两种常见文件名：
      - review.json（官方包解压后通常如此）
      - yelp_academic_dataset_review.json（Kaggle 镜像常见）
    """
    cand = [
        os.path.join(data_dir, "review.json"),
        os.path.join(data_dir, "yelp_academic_dataset_review.json"),
    ]
    for p in cand:
        if os.path.exists(p):
            return p
    raise FileNotFoundError(f"未找到 review.json，请检查目录：{data_dir}")

# ------------------------------
# NEW: 清洗 + k-core
# ------------------------------

def clean_user2seq(user2seq: Dict[int, List[Tuple[int, int]]]) -> Dict[int, List[Tuple[int, int]]]:
    """去掉无效时间戳/重复记录，并按时间升序对每个用户排序。"""
    cleaned = {}
    for u, seq in user2seq.items():
        # 1) 过滤时间戳（>0）
        seq = [(m, ts) for (m, ts) in seq if ts and ts > 0]
        if not seq:
            continue
        # 2) 去重：同一用户-物品若出现多条，保留最新一次
        latest = {}
        for m, ts in seq:
            if (m not in latest) or (ts > latest[m]):
                latest[m] = ts
        seq2 = [(m, ts) for m, ts in latest.items()]
        # 3) 按时间升序
        seq2.sort(key=lambda x: x[1])
        if len(seq2) >= 2:  # 至少保留能形成一条序列的用户
            cleaned[u] = seq2
    return cleaned

def kcore_filter(user2seq: Dict[int, List[Tuple[int, int]]], min_user: int = 10, min_item: int = 10
                 ) -> Dict[int, List[Tuple[int, int]]]:
    """
    全局 k-core（先对正样本子集做，再时间切分）：
    迭代删除：交互次数 < min_user 的用户，以及 < min_item 的物品，直到收敛。
    """
    u2s = {u: list(seq) for u, seq in user2seq.items()}
    changed = True
    while changed:
        changed = False
        # 用户层面
        drop_users = [u for u, seq in u2s.items() if len(seq) < min_user]
        if drop_users:
            for u in drop_users:
                del u2s[u]
            changed = True
        if not u2s:
            break
        # 物品层面
        item_cnt = Counter()
        for seq in u2s.values():
            for m, _ in seq:
                item_cnt[m] += 1
        drop_items = {m for m, c in item_cnt.items() if c < min_item}
        if drop_items:
            for u in list(u2s.keys()):
                new_seq = [(m, ts) for (m, ts) in u2s[u] if m not in drop_items]
                if not new_seq:
                    del u2s[u]
                    changed = True
                else:
                    if len(new_seq) != len(u2s[u]):
                        changed = True
                    u2s[u] = new_seq
    return u2s

# ------------------------------
# main：沿用你的主流程（新增参数与两步预处理）
# ------------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, default="/home/zj/code/yelp/")
    ap.add_argument("--output_dir", type=str, default="/home/zj/code/yelp/")
    ap.add_argument("--max_len", type=int, default=10)
    ap.add_argument("--save_stream", action="store_true", default=True)
    # NEW: 预处理可配
    ap.add_argument("--min_stars", type=float, default=4.0, help="评分阈值，>=该值视为正反馈")
    ap.add_argument("--min_user", type=int, default=10, help="k-core: 每个用户至少出现次数")
    ap.add_argument("--min_item", type=int, default=10, help="k-core: 每个物品至少出现次数")
    args = ap.parse_args()

    review_path = _find_review_file(args.data_dir)
    print(f"[Load] Yelp reviews from: {review_path}")
    user2seq_raw = load_yelp_reviews(review_path, min_stars=args.min_stars)
    n_users_raw = len(user2seq_raw)
    n_inters_raw = sum(len(s) for s in user2seq_raw.values())
    print(f"[Raw] users={n_users_raw}, interactions={n_inters_raw}")

    # --- NEW: 清洗 ---
    print("[Clean] remove ts<=0, dedup per (user,item), sort by time")
    user2seq = clean_user2seq(user2seq_raw)
    n_users_clean = len(user2seq)
    n_inters_clean = sum(len(s) for s in user2seq.values())
    item_set_clean = {m for s in user2seq.values() for (m, _) in s}
    print(f"[After Clean] users={n_users_clean}, items={len(item_set_clean)}, inter={n_inters_clean}")

    # --- NEW: k-core ---
    if args.min_user > 0 or args.min_item > 0:
        print(f"[K-Core] min_user={args.min_user}, min_item={args.min_item} (iterate to convergence)")
        user2seq = kcore_filter(user2seq, args.min_user, args.min_item)
        n_users = len(user2seq)
        n_inters = sum(len(s) for s in user2seq.values())
        item_set = {m for s in user2seq.values() for (m, _) in s}
        print(f"[After K-Core] users={n_users}, items={len(item_set)}, inter={n_inters}")

    print("[Build] examples with global time split ...")
    pack = build_examples_global_time_split(
        user2seq, win_len=args.max_len + 1, stride=5, q_orig=0.5, q_test_start=0.8
    )
    # --- pretty print stats ---
    s = pack["stats"]
    print("[Block Stats]")
    print(f"O: users={s['O']['users']}, items={s['O']['items']}, interactions={s['O']['interactions']}")
    print(f"F: users={s['F']['users']}, items={s['F']['items']}, interactions={s['F']['interactions']}")
    print(f"T: users={s['T']['users']}, items={s['T']['items']}, interactions={s['T']['interactions']}")
    data_original_stride1 = pack["original_stride1"]
    data_original = pack["original"]
    data_finetune = pack["finetune"]
    data_test = pack["test"]
    item_ids = pack["all_items"]

    os.makedirs(args.output_dir, exist_ok=True)
    save_jsonl(os.path.join(args.output_dir, "original.jsonl"), data_original)
    save_jsonl(os.path.join(args.output_dir, "original_stride1.jsonl"), data_original_stride1)
    save_jsonl(os.path.join(args.output_dir, "finetune.jsonl"), data_finetune)
    save_jsonl(os.path.join(args.output_dir, "test.jsonl"), data_test)

    if args.save_stream:
        save_stream(os.path.join(args.output_dir, "original_stream.jsonl"), user2seq, pack["t0"], pack["t1"],
                    which="original")
        save_stream(os.path.join(args.output_dir, "finetune_stream.jsonl"), user2seq, pack["t0"], pack["t1"],
                    which="finetune")

    with open(os.path.join(args.output_dir, "item_ids.json"), "w", encoding="utf-8") as f:
        json.dump({"item_ids": item_ids}, f, ensure_ascii=False)

    with open(os.path.join(args.output_dir, "meta.json"), "w", encoding="utf-8") as f:
        json.dump(
            {
                "counts": {
                    "original": len(data_original),
                    "finetune": len(data_finetune),
                    "test": len(data_test),
                    "num_items": len(item_ids),
                },
                "t0": pack["t0"],
                "t1": pack["t1"],
                "schema": {"prompt": "str", "target": "str", "user_id": "int", "timestamp": "int"},
                "preprocess": {
                    "min_stars": args.min_stars,
                    "kcore": {"min_user": args.min_user, "min_item": args.min_item}
                }
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

if __name__ == "__main__":
    main()
