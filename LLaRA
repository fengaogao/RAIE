# ===== LLaRA 版替换：导入 & 常量 & 公共工具 =====
import os, gc, math, json, random, argparse, re, shutil, time
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from datetime import timedelta
from collections import defaultdict

os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset
from torch.utils.data.distributed import DistributedSampler

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    get_linear_schedule_with_warmup,
)

# Optional PEFT LoRA
try:
    from peft import LoraConfig, get_peft_model, TaskType
    PEFT_AVAILABLE = True
except Exception as e:
    PEFT_AVAILABLE = False
    _PEFT_ERR = e

# Optional clustering（给 RAIE/路由等后续段落用）
try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except Exception:
    SKLEARN_AVAILABLE = False

DATASET_NAME = "ML10M100K"
_TOKEN_RE = re.compile(r"<item_(\d+)>")
BEH_TOKEN = "<BEH>"
SEQ_SEP = " ; "

# ------------------------------
# DDP & 基本工具
# ------------------------------
def is_distributed_env():
    world_size = int(os.environ.get("WORLD_SIZE", "1"))
    return world_size > 1

_GLOO_GROUP = None

def init_distributed():
    if is_distributed_env():
        backend = "nccl" if torch.cuda.is_available() else "gloo"
        dist.init_process_group(backend=backend, init_method="env://",
                                timeout=timedelta(hours=2))
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)

        global _GLOO_GROUP
        if backend == "nccl":
            _GLOO_GROUP = dist.new_group(backend="gloo")
        else:
            _GLOO_GROUP = None

        barrier_gloo()
        return True, rank, world_size, local_rank
    else:
        return False, 0, 1, 0

def _get_gloo_group():
    return _GLOO_GROUP

def barrier_gloo():
    if dist.is_available() and dist.is_initialized():
        g = _get_gloo_group()
        try:
            if g is not None:
                dist.barrier(group=g)
            else:
                dist.barrier()
        except Exception:
            pass

def bcast_object(obj, src=0, use_gloo=True):
    if not (dist.is_available() and dist.is_initialized()):
        return obj
    group = _get_gloo_group() if (use_gloo and _get_gloo_group() is not None) else None
    lst = [obj]
    dist.broadcast_object_list(lst, src=src, group=group)
    return lst[0]

def cleanup_distributed():
    if is_distributed_env() and dist.is_initialized():
        barrier_gloo()
        dist.destroy_process_group()

def set_seed(seed: int = 42, rank: int = 0):
    seed = seed + rank
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p

def aggressive_gc():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

def _free_cuda(*objs):
    for o in objs:
        try:
            if o is None:
                continue
            if hasattr(o, "module"):
                try:
                    delattr(o, "module")
                except Exception:
                    pass
            del o
        except Exception:
            pass
    aggressive_gc()

# ==============================
# I/O & 索引（沿用你现有 JSONL + item_ids.json 流程）
# ==============================
def _tok_to_mid(tok: str) -> int:
    tok = tok.strip()
    m = _TOKEN_RE.fullmatch(tok)
    if m:
        return int(m.group(1))
    tok = tok.lstrip("<").rstrip(">")
    if tok.startswith("item_"):
        tok = tok[len("item_"):]
    return int(tok)

def _load_jsonl(path: str) -> List[dict]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except Exception:
                continue
    return rows

def _build_user_pairs_from_prompt_target(rows: List[dict]) -> Dict[int, List[Tuple[int, int]]]:
    per_user: Dict[int, List[Tuple[int, int]]] = {}
    user_next_ts: Dict[int, int] = {}
    for r in rows:
        try:
            uid = int(r.get("user_id", 0))
        except Exception:
            uid = 0
        pr = (r.get("prompt") or "").strip()
        tg = (r.get("target") or "").strip()
        if not pr or not tg:
            continue
        try:
            mids_ctx = [_tok_to_mid(t) for t in pr.split()]
            mid_tgt = _tok_to_mid(tg)
        except Exception:
            continue
        seq = mids_ctx + [mid_tgt]
        user_next_ts.setdefault(uid, 0)
        for m in seq:
            per_user.setdefault(uid, []).append((m, user_next_ts[uid]))
            user_next_ts[uid] += 1

    for u in per_user:
        seq = sorted(per_user[u], key=lambda x: x[1])
        dedup, seen = [], set()
        for mid, ts in seq:
            if (mid, ts) in seen:
                continue
            seen.add((mid, ts)); dedup.append((mid, ts))
        per_user[u] = dedup
    return per_user

def _read_vocab(item_ids_path: str) -> List[int]:
    with open(item_ids_path, "r", encoding="utf-8") as f:
        obj = json.load(f)
    if isinstance(obj, dict) and "item_ids" in obj:
        item_ids = obj["item_ids"]
    else:
        item_ids = obj
    return [int(x) for x in item_ids]

def _build_base_map_collaborative(item_ids: List[int],
                                  train_pairs: Dict[int, List[Tuple[int, int]]]
                                  ) -> Dict[int, int]:
    pop: Dict[int, int] = defaultdict(int)
    first_ts: Dict[int, int] = {}
    for pairs in train_pairs.values():
        for m, ts in pairs:
            pop[m] += 1
            if m not in first_ts:
                first_ts[m] = ts
    big_ts = 1 << 62
    def sort_key(m):
        return (-pop.get(m, 0), first_ts.get(m, big_ts), m)
    items = sorted(item_ids, key=sort_key)
    return {m: i for i, m in enumerate(items)}

def _build_base_map(item_ids: List[int],
                    train_pairs: Dict[int, List[Tuple[int, int]]],
                    method: str = "sequential",
                    seed: int = 42) -> Dict[int, int]:
    if method == "random":
        items = list(item_ids)
        rng = random.Random(seed)
        rng.shuffle(items)
        return {m: i for i, m in enumerate(items)}
    if method == "collaborative":
        return _build_base_map_collaborative(item_ids, train_pairs)
    first_ts: Dict[int, int] = {}
    for pairs in train_pairs.values():
        for m, ts in pairs:
            if m not in first_ts:
                first_ts[m] = ts
    def sort_key(m):
        if m in first_ts:
            return (0, first_ts[m], m)
        else:
            return (1, 1 << 62, m)
    items = sorted(item_ids, key=sort_key)
    return {m: i for i, m in enumerate(items)}

def _pairs_to_index_sequences(pairs: Dict[int, List[Tuple[int, int]]],
                              mid2idx: Dict[int, int]) -> Dict[int, List[int]]:
    u2s: Dict[int, List[int]] = {}
    for u, lst in pairs.items():
        idx_seq = []
        for mid, _ts in lst:
            if mid in mid2idx:
                idx_seq.append(mid2idx[mid])
        u2s[u] = idx_seq
    return u2s

def load_preprocessed_splits(data_dir: str,
                             item_indexing: str = "sequential",
                             seed: int = 42,
                             min_user_len: int = 5,
                             train_jsonl_path: str = None,
                             item_ids_path: str = None,
                             test_jsonl_path: str = None):
    path_train = train_jsonl_path or os.path.join(data_dir, "original_stride1.jsonl")
    path_vocab = item_ids_path or os.path.join(data_dir, "item_ids.json")
    if not os.path.exists(path_train): raise FileNotFoundError(path_train)
    if not os.path.exists(path_vocab): raise FileNotFoundError(path_vocab)

    rows_train = _load_jsonl(path_train)
    train_pairs = _build_user_pairs_from_prompt_target(rows_train)

    if test_jsonl_path and os.path.exists(test_jsonl_path):
        rows_test = _load_jsonl(test_jsonl_path)
        test_pairs = _build_user_pairs_from_prompt_target(rows_test)
    else:
        test_pairs = {}

    item_ids = _read_vocab(path_vocab)
    if item_indexing not in ("sequential", "random", "collaborative"):
        raise ValueError(f"unknown item_indexing: {item_indexing}")

    method = "random" if item_indexing == "random" else ("collaborative" if item_indexing == "collaborative" else "sequential")
    base_map = _build_base_map(item_ids, train_pairs, method=method, seed=seed)

    train_seq = _pairs_to_index_sequences(train_pairs, base_map)
    test_seq = _pairs_to_index_sequences(test_pairs, base_map) if test_pairs else {}

    def _filter(u2s: Dict[int, List[int]], k: int):
        return {u: s for u, s in u2s.items() if len(s) >= k}
    train_seq = _filter(train_seq, min_user_len)
    test_seq  = _filter(test_seq,  min_user_len) if test_seq else {}
    val_seq   = {u: s[:] for u, s in train_seq.items() if len(s) >= 2}

    n_items = len(item_ids)
    return train_seq, val_seq, test_seq, n_items, base_map

# ------------------------------
# LLaRA 组件：SASRec + LLaRA 数据集/拼接 + 注入/打分
# ------------------------------
class SASRec(nn.Module):
    def __init__(self, n_items, d_model=128, n_heads=2, n_layers=2, max_len=200, dropout=0.1):
        super().__init__()
        self.n_items = n_items
        self.item_emb = nn.Embedding(n_items+1, d_model, padding_idx=0)
        self.pos_emb  = nn.Embedding(max_len, d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,
            dropout=dropout, batch_first=True, activation="gelu"
        )
        self.encoder  = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        self.dropout  = nn.Dropout(dropout)
        self.max_len  = max_len
        nn.init.normal_(self.item_emb.weight, std=0.02)

    def forward(self, seq_ids):
        B, L = seq_ids.shape
        pos = torch.arange(L, device=seq_ids.device).unsqueeze(0).expand(B, L)
        x = self.item_emb(seq_ids) + self.pos_emb(pos)
        x = self.dropout(x)
        attn_mask = (seq_ids==0)
        x = self.encoder(x, src_key_padding_mask=attn_mask)
        return x

    def item_matrix(self):
        return self.item_emb.weight[1:]  # [n_items, d_model]

class SRSimpleDataset(Dataset):
    def __init__(self, user2seq: Dict[int,List[int]], max_len: int):
        self.samples = []
        self.max_len = max_len
        for _, seq in user2seq.items():
            for t in range(1, len(seq)):
                hist = seq[max(0, t-max_len):t]
                tgt  = seq[t]
                self.samples.append((hist, tgt))
    def __len__(self): return len(self.samples)
    def __getitem__(self, idx):
        hist, tgt = self.samples[idx]
        return torch.tensor(hist, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)

def sr_collate(batch, n_items, max_len):
    B = len(batch)
    lens = [len(h) for h,_ in batch]
    L = min(max(lens), max_len)
    seq = torch.zeros(B, L, dtype=torch.long)
    tgt = torch.zeros(B, dtype=torch.long)
    for i, (h, t) in enumerate(batch):
        h = h[-L:]
        seq[i, -len(h):] = torch.tensor(h) + 1
        tgt[i] = t + 1
    return seq, tgt

def train_sasrec(model, loader, device, epochs=5, lr=1e-3, fp16=False, is_main=True):
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scaler = torch.amp.GradScaler('cuda', enabled=fp16 and torch.cuda.is_available())
    for ep in range(1, epochs+1):
        if isinstance(loader.sampler, DistributedSampler):
            loader.sampler.set_epoch(ep)
        model.train(); total = 0
        itr = tqdm(loader, desc=f"SASRec Train ep{ep}", disable=not is_main)
        for seq, tgt in itr:
            seq, tgt = seq.to(device), tgt.to(device)
            with torch.amp.autocast('cuda', enabled=fp16 and torch.cuda.is_available()):
                h = model(seq)
                last = h[:, -1, :]
                W = model.item_emb.weight
                logits = last @ W.t()
                loss = F.cross_entropy(logits[:,1:], tgt-1)
            scaler.scale(loss).backward()
            scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)
            total += loss.item()
        if is_main:
            print(f"[SASRec] ep={ep} loss={total/max(1,len(loader)):.4f}")
    return model

@dataclass
class LLaRAExample:
    input_ids: List[int]
    attention_mask: List[int]
    labels: List[int]
    beh_positions: List[int]
    beh_item_idx: List[int]
    hist_idx: List[int]
    target_idx: int
    prompt_len: int

# ===== 新增：保存“干净 base”（含 <BEH> 的词表） =====
def save_clean_base_with_beh_token(peft_wrapped, tokenizer, out_dir: str, orig_base_path: str):
    """
    peft_wrapped: 已包上 LoRA 的模型（或原模型），我们只用它来读取当前 embedding（包含新增 token 的大小）
    tokenizer:    当前 tokenizer（包含 <BEH>）
    out_dir:      导出目录
    orig_base_path: 原始基座模型（未加 LoRA）的路径，用于加载“干净 base”
    """
    ensure_dir(out_dir)
    # 1) 加载干净的 base
    base = AutoModelForCausalLM.from_pretrained(orig_base_path, low_cpu_mem_usage=True)
    if hasattr(base.config, "use_cache"):
        base.config.use_cache = False

    # 2) 确保 tokenizer 有 pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 3) 对齐词表大小
    vocab_size = len(tokenizer)
    try:
        base.resize_token_embeddings(vocab_size, mean_resizing=True)
    except TypeError:
        base.resize_token_embeddings(vocab_size)

    # 4) 从 peft_wrapped 读取当前 embedding，拷贝到 base
    with torch.no_grad():
        src_in = (peft_wrapped.module if hasattr(peft_wrapped, "module") else peft_wrapped).get_input_embeddings()
        dst_in = base.get_input_embeddings()
        dst_in.weight.data[:src_in.weight.data.shape[0]].copy_(src_in.weight.data)

        # 输出头（很多模型 tied weights；如果没绑，则尽量拷贝过去）
        try:
            # 如果 tied，下面这个 set_output_embeddings(None) 可能无效；我们直接尝试覆盖 lm_head
            if hasattr(base, "lm_head") and base.lm_head is not None and hasattr(base.lm_head, "weight"):
                base.lm_head.weight.data[:src_in.weight.data.shape[0]].copy_(src_in.weight.data)
        except Exception:
            pass

    # 5) 一并保存 tokenizer
    tokenizer.save_pretrained(out_dir)
    base.save_pretrained(out_dir)


class LLaRADatasetFromJsonl(Dataset):
    def __init__(self, jsonl_path: str, mid2idx: Dict[int,int],
                 tokenizer, idx2title: List[str],
                 max_history: int = 50, use_hybrid: bool = True):
        self.tok = tokenizer
        self.max_history = max_history
        self.use_hybrid = use_hybrid
        self.examples: List[LLaRAExample] = []

        rows = _load_jsonl(jsonl_path)
        beh_id = self.tok.convert_tokens_to_ids(BEH_TOKEN)
        for r in rows:
            pr = (r.get("prompt") or "").strip()
            tg = (r.get("target") or "").strip()
            if not pr or not tg: continue
            try:
                mids_ctx = [_tok_to_mid(t) for t in pr.split()]
                mid_tgt  = _tok_to_mid(tg)
            except Exception:
                continue
            idx_ctx = [mid2idx[m] for m in mids_ctx if m in mid2idx]
            if not idx_ctx or (mid_tgt not in mid2idx): continue
            idx_ctx = idx_ctx[-max_history:]
            tgt_idx = mid2idx[mid_tgt]

            titles = [idx2title[i] for i in idx_ctx]
            if use_hybrid:
                hist_text = SEQ_SEP.join([f"{ti} {BEH_TOKEN}" for ti in titles])
            else:
                hist_text = SEQ_SEP.join(titles)
            prompt = f"Given the {DATASET_NAME} user's interaction history: {hist_text}. Predict the next item the user will consume.\nAnswer:"
            ans = " " + idx2title[tgt_idx]

            enc_p = self.tok(prompt, add_special_tokens=True)
            enc_a = self.tok(ans, add_special_tokens=False)
            input_ids = enc_p["input_ids"] + enc_a["input_ids"]
            attention = enc_p["attention_mask"] + enc_a["attention_mask"]
            labels = [-100]*len(enc_p["input_ids"]) + enc_a["input_ids"]

            beh_positions, beh_item_idx = [], []
            if use_hybrid:
                ids = enc_p["input_ids"]
                for j, tid in enumerate(ids):
                    if tid == beh_id:
                        beh_positions.append(j)
                beh_item_idx = idx_ctx[:len(beh_positions)]

            self.examples.append(LLaRAExample(
                input_ids=input_ids,
                attention_mask=attention,
                labels=labels,
                beh_positions=beh_positions,
                beh_item_idx=beh_item_idx,
                hist_idx=idx_ctx,
                target_idx=tgt_idx,
                prompt_len=len(enc_p["input_ids"])
            ))

    def __len__(self): return len(self.examples)
    def __getitem__(self, i):
        e = self.examples[i]
        return {
            "input_ids": torch.tensor(e.input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(e.attention_mask, dtype=torch.long),
            "labels": torch.tensor(e.labels, dtype=torch.long),
            "beh_positions": torch.tensor(e.beh_positions, dtype=torch.long),
            "beh_item_idx": torch.tensor(e.beh_item_idx, dtype=torch.long),
            "target_idx": torch.tensor(e.target_idx, dtype=torch.long),
            "prompt_len": torch.tensor(e.prompt_len, dtype=torch.long),
        }

def llara_collate(batch, pad_id):
    max_len = max(len(x["input_ids"]) for x in batch)
    def pad(seq, val): return seq + [val]*(max_len-len(seq))
    input_ids = torch.tensor([pad(x["input_ids"].tolist(), pad_id) for x in batch], dtype=torch.long)
    attention = torch.tensor([pad(x["attention_mask"].tolist(), 0) for x in batch], dtype=torch.long)
    labels = torch.tensor([pad(x["labels"].tolist(), -100) for x in batch], dtype=torch.long)
    beh_pos_list, beh_idx_list = [], []
    target_idx = torch.tensor([x["target_idx"].item() for x in batch], dtype=torch.long)
    prompt_len = torch.tensor([x["prompt_len"].item() for x in batch], dtype=torch.long)
    for x in batch:
        beh_pos_list.append(x["beh_positions"].tolist())
        beh_idx_list.append(x["beh_item_idx"].tolist())
    return {
        "input_ids": input_ids, "attention_mask": attention, "labels": labels,
        "beh_positions_list": beh_pos_list, "beh_item_idx_list": beh_idx_list,
        "target_idx": target_idx, "prompt_len": prompt_len
    }

# ===== 新增：简易 ReplayBuffer（DDP 友好，可循环迭代） =====
class ReplayBuffer:
    def __init__(self, dataset, collate_fn, batch_size: int, sampler=None, num_workers: int = 2, pin_memory: bool = True):
        """
        dataset:     复习用的数据集（你这里传 ds_O）
        collate_fn:  与训练时相同的 collate（llara_collate）
        batch_size:  一次取多少条
        sampler:     可传 DistributedSampler 以支持 DDP
        """
        self.dataset = dataset
        self.collate_fn = collate_fn
        self.batch_size = batch_size
        self.sampler = sampler
        self.num_workers = num_workers
        self.pin_memory = pin_memory

        self._build_loader()

    def _build_loader(self):
        self.loader = DataLoader(
            self.dataset,
            batch_size=self.batch_size,
            shuffle=(self.sampler is None),
            sampler=self.sampler,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=self.collate_fn,
            drop_last=False
        )
        self._it = iter(self.loader)

    def set_epoch(self, epoch: int):
        """DDP 下需要把 epoch 传给 DistributedSampler 以改变随机顺序。"""
        if isinstance(self.sampler, DistributedSampler):
            self.sampler.set_epoch(epoch)
        self._build_loader()

    def get_batch(self):
        """取下一批；用尽后自动从头再来（循环复用）。"""
        try:
            batch = next(self._it)
        except StopIteration:
            self._build_loader()
            batch = next(self._it)
        return batch

# ===== 新增：F 段微调循环（支持 replay） =====
def _concat_llara_batches(a: dict, b: dict):
    """把两个 llara_collate 后的 batch 合并（张量拼接 + list 级联）。"""
    out = {}
    for k, v in a.items():
        if isinstance(v, torch.Tensor):
            out[k] = torch.cat([a[k], b[k]], dim=0)
        elif isinstance(v, list):
            out[k] = a[k] + b[k]
        else:
            # 其它类型（一般不会出现），回退到前者
            out[k] = a[k]
    return out

def finetune_one_epoch_llara(model, tok, sr2llm, item_table, loader_F, device,
                             optimizer, scheduler=None, fp16=False,
                             plugin="none", replay_buf: Optional['ReplayBuffer']=None,
                             replay_ratio: float=0.0, grad_clip: float=1.0, log_every: int=100):
    """
    单个 epoch 的微调。
    - model:      可能是 DDP 包裹的 PEFT 模型
    - tok:        tokenizer
    - sr2llm:     映射网络（可能是 DDP）
    - item_table: [n_items, d_sas] —— **建议在调用前就放到 device 上**（见你 stage 里的做法）
    - loader_F:   F 段 DataLoader（已是 DDP-aware）
    - optimizer:  AdamW
    - scheduler:  线性 warmup 等
    - plugin:     "none" 或 "replay"
    - replay_buf: ReplayBuffer，plugin="replay" 时必传
    - replay_ratio: 0.3 表示 混入比例 = replay_batch_size / (replay_batch_size + finetune_batch_size)
    """
    model.train(); sr2llm.train()
    scaler = torch.amp.GradScaler('cuda', enabled=fp16 and torch.cuda.is_available())

    # 用于梯度裁剪的参数集合
    train_params = [p for p in model.parameters() if p.requires_grad] + \
                   [p for p in sr2llm.parameters() if p.requires_grad]

    running, nstep = 0.0, 0
    start = time.time()

    for step, batch_F in enumerate(loader_F, start=1):
        # 1) 取 F 段 batch
        mixed_batch = batch_F

        # 2) 可选：混入 replay batch
        if plugin == "replay" and replay_buf is not None and replay_ratio > 0.0:
            # 计算要混入的 O 段 batch 数量（这里用“1:1 按比例合并”，简单起见取一个 batch）
            batch_R = replay_buf.get_batch()
            # 简单按比例拼接：控制最终混入占比接近 replay_ratio
            if replay_ratio >= 0.5:
                # R:F ≈ 1:1（R 多一些）
                mixed_batch = _concat_llara_batches(batch_R, batch_F)
            else:
                # F:R ≈ 1:1（F 多一些）
                mixed_batch = _concat_llara_batches(batch_F, batch_R)

        # 3) 前向 + 反传
        loss = lm_loss_llara(model, tok, sr2llm, item_table, mixed_batch, device, fp16=fp16)
        optimizer.zero_grad(set_to_none=True)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        if grad_clip and grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(train_params, grad_clip)

        scaler.step(optimizer); scaler.update()
        if scheduler is not None:
            scheduler.step()

        running += float(loss.detach().item())
        nstep += 1
        if (log_every is not None) and (log_every > 0) and (step % log_every == 0):
            avg = running / max(1, nstep)
            print(f"[F] step={step} avg_loss={avg:.4f} time={time.time()-start:.1f}s", flush=True)

    return running / max(1, nstep)

# ------------------------------
# LLaRA 前向：注入 & 损失 & 评测
# ------------------------------
def inject_behavior_embeddings(llm, tok, sr2llm, item_table, batch, device):
    base = llm.module if hasattr(llm, "module") else llm
    emb_llm = base.get_input_embeddings()

    input_ids = batch["input_ids"].to(device, non_blocking=True)
    inputs_embeds = emb_llm(input_ids)

    beh_pos_list = batch["beh_positions_list"]
    beh_idx_list = batch["beh_item_idx_list"]

    # 扁平化收集所有 (b, pos) 与对应 item_idx
    rows, cols, flat_items = [], [], []
    for b, (posL, idxL) in enumerate(zip(beh_pos_list, beh_idx_list)):
        if not posL:
            continue
        rows.extend([b] * len(posL))
        cols.extend(posL)
        flat_items.extend(idxL)

    if flat_items:
        # 一次性取 item embeddings -> 投影 -> scatter 回去
        idx_t = torch.as_tensor(flat_items, device=device, dtype=torch.long)
        items = item_table[idx_t]                     # [N, d_sas]
        proj  = sr2llm(items)                         # [N, d_llm]
        proj = proj.to(inputs_embeds.dtype)

        r = torch.as_tensor(rows, device=device, dtype=torch.long)
        c = torch.as_tensor(cols, device=device, dtype=torch.long)
        inputs_embeds[r, c] = proj

    return inputs_embeds


def lm_loss_llara(llm, tok, sr2llm, item_table, batch, device, fp16=False):
    input_ids = batch["input_ids"].to(device, non_blocking=True)
    attention = batch["attention_mask"].to(device, non_blocking=True)
    labels = batch["labels"].to(device, non_blocking=True)
    inputs_embeds = inject_behavior_embeddings(llm, tok, sr2llm, item_table, batch, device)
    with torch.amp.autocast('cuda', enabled=fp16 and torch.cuda.is_available()):
        out = llm(inputs_embeds=inputs_embeds, attention_mask=attention, labels=labels)
        loss = out.loss
    return loss

def build_idx2title(n_items: int) -> List[str]:
    return [f"Item {i}" for i in range(n_items)]

@torch.no_grad()
def build_prompt_embeds_for_one(llm, tok, sr2llm, item_table, titles, hist_idx, device):
    hist_text = SEQ_SEP.join([f"{ti} {BEH_TOKEN}" for ti in titles])
    prompt = f"Given the {DATASET_NAME} user's interaction history: {hist_text}. Predict the next item the user will consume.\nAnswer:"
    enc_p = tok(prompt, add_special_tokens=True, return_tensors="pt")
    input_ids = enc_p["input_ids"].to(device)
    attention = enc_p["attention_mask"].to(device)
    emb = (llm.module if hasattr(llm, "module") else llm).get_input_embeddings()(input_ids)
    beh_id = tok.convert_tokens_to_ids(BEH_TOKEN)
    ids_list = enc_p["input_ids"][0].tolist()
    beh_positions = [i for i,t in enumerate(ids_list) if t==beh_id]
    for pos, item_idx in zip(beh_positions, hist_idx[:len(beh_positions)]):
        v = item_table[item_idx:item_idx+1].to(device)
        v = sr2llm(v)
        v = v.to(emb.dtype)
        emb[0, pos, :] = v
    return input_ids, attention, emb

@torch.no_grad()
def score_candidates_llm_batched(llm, tok, prompt_ids, prompt_attn, prompt_embeds,
                                 cand_token_ids, device, fp16=False, chunk_size=256):
    # 开启 cache 计算 prompt 的 KV
    base = llm.module if hasattr(llm, "module") else llm
    if hasattr(base.config, "use_cache"): base.config.use_cache = True

    with torch.amp.autocast('cuda', enabled=fp16 and torch.cuda.is_available()):
        out_p = base(inputs_embeds=prompt_embeds, attention_mask=prompt_attn, use_cache=True)
    pkv = out_p.past_key_values

    # 在新版本 transformers 中，模型需要 Cache 对象而不是 tuple。
    # 这里兼容旧缓存格式，避免 AttributeError: 'tuple' object has no attribute 'get_seq_length'.
    try:
        from transformers.cache_utils import DynamicCache
    except Exception:
        DynamicCache = None

    if isinstance(pkv, tuple) and DynamicCache is not None:
        pkv = DynamicCache.from_legacy_cache(pkv)

    scores = []
    # 分块批处理候选，避免显存爆
    for i in range(0, len(cand_token_ids), chunk_size):
        ids = cand_token_ids[i:i+chunk_size]
        # padding 到同长
        maxL = max(x.shape[1] for x in ids)
        pad_id = tok.pad_token_id
        batch_ids = torch.full((len(ids), maxL), pad_id, dtype=torch.long, device=device)
        batch_attn = torch.zeros_like(batch_ids)
        for r, t in enumerate(ids):
            L = t.shape[1]
            batch_ids[r, :L] = t.to(device)
            batch_attn[r, :L] = 1

        # 复制/扩展 prompt 的 KV 到 batch 维
        def _tile_pkv(pkv, times):
            # 新 cache 接口提供了批量复制方法
            if hasattr(pkv, "batch_repeat_interleave"):
                return pkv.batch_repeat_interleave(times)

            # 兼容旧 tuple 缓存格式
            new = []
            for layer in pkv:
                k, v = layer[0], layer[1]
                # [B=1, H, T, D] -> [B=times, H, T, D]
                new.append((k.expand(times, *k.shape[1:]).contiguous(),
                            v.expand(times, *v.shape[1:]).contiguous()))
            return tuple(new)
        pkv_b = _tile_pkv(pkv, batch_ids.size(0))

        with torch.amp.autocast('cuda', enabled=fp16 and torch.cuda.is_available()):
            out = base(input_ids=batch_ids, attention_mask=batch_attn,
                       past_key_values=pkv_b, use_cache=False)
            logits = out.logits[:, :-1, :]
            labels = batch_ids[:, 1:]
            logprob = F.log_softmax(logits, dim=-1).gather(-1, labels.unsqueeze(-1)).squeeze(-1)
            # 每个候选答案 token 的 logprob 求和
            s = logprob.sum(dim=1)
        scores.extend(s.tolist())
    return scores


def recall_ndcg_from_ranks(rank: int, K_list=(5,10,20)):
    out={}
    for k in K_list:
        out[f"Recall@{k}"] = 1.0 if rank <= k else 0.0
        out[f"NDCG@{k}"] = 0.0 if rank>k else 1.0/math.log2(rank+1)
    return out

@torch.no_grad()
def evaluate_llara_dataset(llm, tok, sr2llm, item_table, dataset: LLaRADatasetFromJsonl,
                           idx2title: List[str], k_list, device, fp16=False, is_main=True,
                           max_eval=None, cand_chunk_size: int = 256):
    if not is_main:
        return None
    # 确保评测时关闭 dropout 等训练态
    prev_llm_mode = llm.training
    prev_sr_mode = sr2llm.training if hasattr(sr2llm, "training") else None
    llm.eval()
    if hasattr(sr2llm, "eval"):
        sr2llm.eval()
    K_list = sorted(set(k_list))
    sums = {f"Recall@{k}":0.0 for k in K_list}
    sums.update({f"NDCG@{k}":0.0 for k in K_list})
    n = 0

    # 构建一次全量标题分词缓存，后续按候选 id 取切片，避免重复 tokenizer 调用
    title_token_cache = [tok(" "+t, add_special_tokens=False, return_tensors="pt")["input_ids"]
                         for t in idx2title]

    pairs = [(e.hist_idx, e.target_idx) for e in dataset.examples]
    if max_eval is not None and max_eval > 0:
        pairs = pairs[:max_eval]

    it = tqdm(pairs, desc="Eval", disable=not is_main)
    for hist_idx, tgt in it:
        # 1) 历史构造 prompt（含行为注入）
        hist_used = hist_idx[-50:]
        titles = [idx2title[i] for i in hist_used]
        p_ids, p_attn, p_emb = build_prompt_embeds_for_one(llm, tok, sr2llm, item_table, titles, hist_used, device)

        # 2) 预筛 top-M 候选（向量相似度）
        cand_ids = prefilter_candidates(hist_used, item_table, M=getattr(dataset, "eval_prefilter", 300))
        if tgt not in cand_ids:
            # 保底：确保正样本在候选里
            cand_ids = cand_ids[:-1] + [tgt]

        # 3) 从缓存中取候选分词，批量打分（KV cache）
        cand_token_ids = [title_token_cache[i] for i in cand_ids]
        scores = score_candidates_llm_batched(llm, tok, p_ids, p_attn, p_emb,
                                              cand_token_ids, device, fp16=fp16, chunk_size=cand_chunk_size)

        # 4) 本地排序转名次
        order_local = np.argsort(scores)[::-1]
        ranked_ids = np.array(cand_ids)[order_local]
        # rank 从 1 开始
        rank = int(np.where(ranked_ids == tgt)[0][0]) + 1

        # 5) 累加指标
        for k in K_list:
            sums[f"Recall@{k}"] += 1.0 if rank <= k else 0.0
            sums[f"NDCG@{k}"]   += (0.0 if rank > k else 1.0 / math.log2(rank + 1))
        n += 1
    metrics = {k: (v / max(1, n)) for k, v in sums.items()}

    # 还原训练/评测模式
    if prev_llm_mode:
        llm.train()
    if prev_sr_mode is not None:
        if prev_sr_mode:
            sr2llm.train()
    return metrics


@torch.no_grad()
def prefilter_candidates(hist_idx, item_table, M=300):
    # hist 平均向量
    h = item_table[hist_idx].mean(dim=0, keepdim=True)  # [1, d]
    sims = (item_table @ h.t()).squeeze(1)             # [n_items]
    topk = torch.topk(sims, k=min(M, item_table.size(0)), largest=True).indices.tolist()
    return topk

# ====== RAIE: 1) KMeans 分区 & 赋予每个 item 的 region id ======
def build_item_regions(item_table: torch.Tensor, k: int, seed: int = 42):
    """
    item_table: [n_items, d_sas]（**CPU 或 GPU 都行**，本函数内部会搬到 CPU 做 KMeans）
    return:
        item2region:  [n_items]  的 numpy int 数组
        centers:      [k, d_sas] 的 torch.Tensor（float32, cpu）
    """
    if item_table.is_cuda:
        X = item_table.detach().cpu().numpy()
    else:
        X = item_table.detach().numpy()

    if not SKLEARN_AVAILABLE:
        raise RuntimeError("scikit-learn 不可用，无法做 KMeans。请安装或把 --stage=raie 改回占位。")

    kmeans = KMeans(n_clusters=k, n_init=10, random_state=seed, verbose=0)
    labels = kmeans.fit_predict(X)
    centers = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32, device='cpu')
    return labels.astype(np.int64), centers  # numpy int64, torch fp32


# ====== RAIE: 2) 依据“样本的路由特征”选择区域 ======
def assign_region_for_hist(hist_idx: List[int], item_table: torch.Tensor, centers: torch.Tensor, top_m: int = 50):
    """
    hist_idx: 历史 item 的 index 列表（用 mid2idx 编好号）
    item_table: [n_items, d_sas]（**应在 cpu 或 gpu 都可，这里统一拿到 cpu**）
    centers:    [k, d_sas]（cpu）
    策略：取最近 top_m 个历史（不足则全用），求其 embedding 的均值，选最近的质心（L2）。
    """
    if len(hist_idx) == 0:
        return 0  # 兜底：路由到 0 号区

    use_idx = hist_idx[-top_m:]
    with torch.no_grad():
        if item_table.is_cuda:
            H = item_table[use_idx].detach().cpu()
        else:
            H = item_table[use_idx].detach().clone()
        h_mean = H.mean(dim=0, keepdim=True)  # [1, d]
        # L2 距离最近中心
        d2 = torch.cdist(h_mean, centers, p=2.0)  # [1, k]
        rid = int(torch.argmin(d2, dim=1).item())
    return rid


# ====== RAIE: 3) 按区域切分数据集（返回每区 Subset） ======
def split_dataset_by_region(ds_F: 'LLaRADatasetFromJsonl',
                            item_table: torch.Tensor,
                            centers: torch.Tensor) -> List[Subset]:
    """
    以 **样本的历史 hist_idx** 为依据做路由（更鲁棒，不偷看 target）。
    返回长度为 K 的 Subset 列表，每个只含该区域样本的 indices。
    """
    # centers 在 cpu
    K = centers.size(0)
    buckets = [[] for _ in range(K)]
    for idx, e in enumerate(ds_F.examples):
        rid = assign_region_for_hist(e.hist_idx, item_table, centers, top_m=50)
        buckets[rid].append(idx)
    subsets = [Subset(ds_F, idxs) for idxs in buckets]
    return subsets


# ====== RAIE: 4) 评测时根据历史路由，动态切换适配器 ======
@torch.no_grad()
def evaluate_llara_raie(llm_peft, adapter_names: List[str], tok, sr2llm, item_table, dataset: 'LLaRADatasetFromJsonl',
                        idx2title: List[str], k_list, device, centers_cpu: torch.Tensor,
                        fp16=False, is_main=True, max_eval=None, cand_chunk_size: int = 256):
    if not is_main:
        return None
    prev_llm_mode = llm_peft.training
    prev_sr_mode = sr2llm.training if hasattr(sr2llm, "training") else None
    llm_peft.eval()
    if hasattr(sr2llm, "eval"):
        sr2llm.eval()
    base = llm_peft.module if hasattr(llm_peft, "module") else llm_peft
    K_list = sorted(set(k_list))
    sums = {f"Recall@{k}":0.0 for k in K_list}
    sums.update({f"NDCG@{k}":0.0 for k in K_list})
    n = 0

    # 标题分词缓存
    title_token_cache = [tok(" "+t, add_special_tokens=False, return_tensors="pt")["input_ids"]
                         for t in idx2title]

    pairs = [(e.hist_idx, e.target_idx) for e in dataset.examples]
    if max_eval is not None and max_eval > 0:
        pairs = pairs[:max_eval]

    it = tqdm(pairs, desc="Eval-RAIE", disable=not is_main)
    centers = centers_cpu  # cpu tensor
    for hist_idx, tgt in it:
        # 1) 路由选择区域适配器
        rid = assign_region_for_hist(hist_idx, item_table, centers, top_m=50)
        base.set_adapter(adapter_names[rid])

        # 2) 准备 prompt（含行为注入）
        hist_used = hist_idx[-50:]
        titles = [idx2title[i] for i in hist_used]
        p_ids, p_attn, p_emb = build_prompt_embeds_for_one(base, tok, sr2llm, item_table, titles, hist_used, device)

        # 3) 区域内同样做预筛（用全库/区域库都可，这里沿用全库相似度预筛）
        cand_ids = prefilter_candidates(hist_used, item_table, M=getattr(dataset, "eval_prefilter", 300))
        if tgt not in cand_ids:
            cand_ids = cand_ids[:-1] + [tgt]

        cand_token_ids = [title_token_cache[i] for i in cand_ids]
        scores = score_candidates_llm_batched(base, tok, p_ids, p_attn, p_emb,
                                              cand_token_ids, device, fp16=fp16, chunk_size=cand_chunk_size)

        order_local = np.argsort(scores)[::-1]
        ranked_ids = np.array(cand_ids)[order_local]
        rank = int(np.where(ranked_ids == tgt)[0][0]) + 1

        for k in K_list:
            sums[f"Recall@{k}"] += 1.0 if rank <= k else 0.0
            sums[f"NDCG@{k}"]   += (0.0 if rank > k else 1.0 / math.log2(rank + 1))
        n += 1
    metrics = {k: (v / max(1, n)) for k, v in sums.items()}

    if prev_llm_mode:
        llm_peft.train()
    if prev_sr_mode is not None:
        if prev_sr_mode:
            sr2llm.train()
    return metrics


# ------------------------------
# mid2idx 保存/加载 & LLaRA 阶段数据集构造
# ------------------------------
def save_mid2idx(path: str, mid2idx: Dict[int, int]):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump({str(k): int(v) for k, v in mid2idx.items()}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[WARN] save mid2idx failed: {e}")

def load_mid2idx(path: str) -> Optional[Dict[int, int]]:
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        return {int(k): int(v) for k, v in obj.items()}
    except Exception as e:
        print(f"[WARN] load mid2idx failed: {e}")
        return None

def build_or_load_mid2idx(args, is_main=True) -> Tuple[Dict[int,int], int]:
    mid2idx_path = os.path.join(args.output_dir, "mid2idx.json")
    mid2idx = load_mid2idx(mid2idx_path)
    item_ids = _read_vocab(args.item_ids_path or os.path.join(args.data_dir, "item_ids.json"))
    n_items = len(item_ids)
    if mid2idx is not None:
        if is_main:
            print(f"[INFO] Loaded mid2idx from {mid2idx_path} (|mid|={len(mid2idx)})")
        return mid2idx, n_items

    if is_main:
        print("[INFO] mid2idx.json not found -> rebuild mapping from training split.")
    _, _, _, n_items2, mid2idx = load_preprocessed_splits(
        data_dir=args.data_dir,
        item_indexing=args.item_indexing,
        seed=args.seed,
        min_user_len=args.min_user_len,
        train_jsonl_path=args.train_jsonl_path or os.path.join(args.data_dir, "original_stride1.jsonl"),
        item_ids_path=args.item_ids_path or os.path.join(args.data_dir, "item_ids.json"),
        test_jsonl_path=None
    )
    if (mid2idx is not None) and (len(mid2idx) > 0):
        if is_main:
            save_mid2idx(mid2idx_path, mid2idx)
            print(f"[SAVE] mid2idx saved to {mid2idx_path}")
    assert n_items == n_items2, "item_ids 长度与重建得到的不一致，请检查数据一致性。"
    return mid2idx, n_items

def make_llara_datasets_for_stage(args, tok, mid2idx, n_items, idx2title):
    def make_ds(jsonl_path):
        return LLaRADatasetFromJsonl(
            jsonl_path, mid2idx, tok, idx2title,
            max_history=args.max_history, use_hybrid=True
        )
    train_jsonl_path = args.train_jsonl_path or os.path.join(args.data_dir, "original_stride1.jsonl")
    original_jsonl_path = args.original_jsonl_path or os.path.join(args.data_dir, "original.jsonl")
    test_jsonl_path     = args.test_jsonl_path or os.path.join(args.data_dir, "test.jsonl")
    finetune_jsonl_path = args.finetune_jsonl_path or os.path.join(args.data_dir, "finetune.jsonl")
    if not os.path.exists(finetune_jsonl_path):
        raise FileNotFoundError(f"finetune_jsonl_path not found: {finetune_jsonl_path}")

    ds_trainO = make_ds(train_jsonl_path)
    ds_O = make_ds(original_jsonl_path)
    ds_T = make_ds(test_jsonl_path)
    ds_F = make_ds(finetune_jsonl_path)
    return ds_trainO, ds_O, ds_T, ds_F


# ==============================
# ====== 分阶段执行（LLaRA） =====
# ==============================

def stage_pre(args, device, is_distributed, local_rank, is_main, load_dtype):
    # 1) 构建索引与序列（与原版一致）
    train_seq, val_seq, test_seq, n_items, mid2idx = load_preprocessed_splits(
        data_dir=args.data_dir,
        item_indexing=args.item_indexing,
        seed=args.seed,
        min_user_len=args.min_user_len,
        train_jsonl_path=args.train_jsonl_path or os.path.join(args.data_dir, "original_stride1.jsonl"),
        item_ids_path=args.item_ids_path or os.path.join(args.data_dir, "item_ids.json"),
        test_jsonl_path=None
    )
    if is_main:
        print(f"[Data] items={n_items} | train_users={len(train_seq)} | val_users={len(val_seq)}")

    # 2) 训练 SASRec（O 段）
    if is_main:
        print("[SASRec] Building & training ...")
    sas = SASRec(n_items=n_items, d_model=args.d_sas, n_heads=2, n_layers=2, max_len=args.max_history).to(device)
    ds_sr = SRSimpleDataset(train_seq, max_len=args.max_history)
    sampler_sr = DistributedSampler(ds_sr, shuffle=True) if is_distributed else None
    dl_sr = DataLoader(ds_sr, batch_size=args.batch_size, shuffle=(sampler_sr is None),
                       sampler=sampler_sr,
                       collate_fn=lambda b: sr_collate(b, n_items, args.max_history), num_workers=2, pin_memory=True)
    sas = train_sasrec(sas, dl_sr, device, epochs=args.epochs_sasrec, lr=args.lr_sas, fp16=args.fp16, is_main=is_main)
    item_table = sas.item_matrix().detach().cpu()  # [n_items, d_sas]
    if is_main:
        ensure_dir(args.output_dir)
        torch.save(item_table, os.path.join(args.output_dir, "sasrec_item_table.pt"))
        print(f"[SAVE] SASRec item embeddings -> {os.path.join(args.output_dir, 'sasrec_item_table.pt')}")

    if is_distributed:
        barrier_gloo()
        # 广播 item_table（所有 rank 获取）
        if is_distributed:
            barrier_gloo()
            if is_main:
                torch.save(item_table.cpu(), os.path.join(args.output_dir, "sasrec_item_table.pt"))
            barrier_gloo()
            item_table = torch.load(os.path.join(args.output_dir, "sasrec_item_table.pt"), map_location='cpu')

    if is_main:
        print("[Model] Loading tokenizer and model ...")
    tok = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    if tok.convert_tokens_to_ids(BEH_TOKEN) == tok.unk_token_id:
        tok.add_tokens([BEH_TOKEN], special_tokens=False)

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,
        low_cpu_mem_usage=True,
        dtype=load_dtype,
        attn_implementation="sdpa"  # 环境无 FA2 时 transformers 会回退/报错；若报错可改为 "sdpa"
    )

    model.resize_token_embeddings(len(tok), mean_resizing=True)
    if args.grad_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()

    # LoRA 包装
    if args.use_lora:
        if not PEFT_AVAILABLE:
            raise RuntimeError(f"PEFT not available: {_PEFT_ERR}")
        peft_cfg = LoraConfig(
            r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
            bias='none', task_type=TaskType.CAUSAL_LM,
            target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
        )
        model = get_peft_model(model, peft_cfg)
    model.to(device)
    if hasattr(model.config, "use_cache"):
        model.config.use_cache = False

    # SR2LLM 投影
    d_llm = (model.module if hasattr(model, "module") else model).get_input_embeddings().weight.shape[1]
    sr2llm = nn.Sequential(
        nn.Linear(args.d_sas, d_llm),
        nn.GELU(),
        nn.Linear(d_llm, d_llm)
    ).to(device)

    if is_distributed:
        model = torch.nn.parallel.DistributedDataParallel(
            model, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None,
            find_unused_parameters=False
        )
        sr2llm = torch.nn.parallel.DistributedDataParallel(
            sr2llm, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None,
            find_unused_parameters=False
        )

    # 4) LLaRA 数据（O / O_eval / T_eval / F）
    mid2idx_path = os.path.join(args.output_dir, "mid2idx.json")
    if is_main:
        save_mid2idx(mid2idx_path, mid2idx)
    if is_distributed:
        barrier_gloo()

    idx2title = build_idx2title(n_items)
    ds_trainO, ds_O, ds_T, ds_F = make_llara_datasets_for_stage(args, tok, mid2idx, n_items, idx2title)

    pad_id = tok.pad_token_id
    collate = lambda batch: llara_collate(batch, pad_id)

    train_sampler = DistributedSampler(ds_trainO, num_replicas=dist.get_world_size() if is_distributed else 1,
                                       rank=dist.get_rank() if is_distributed else 0, shuffle=True) if is_distributed else None
    train_loader = DataLoader(ds_trainO, batch_size=args.batch_size,
                              shuffle=(train_sampler is None), sampler=train_sampler,
                              num_workers=2, pin_memory=True, collate_fn=collate)

    ld_O = ds_O
    ld_T = ds_T

    # 优化器：LoRA 参数 + SR2LLM 参数（O 段）
    params = [p for p in model.parameters() if p.requires_grad] + [p for p in sr2llm.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(params, lr=args.lr, weight_decay=args.weight_decay)
    num_train_steps = args.epochs * max(1, len(train_loader))
    scheduler = get_linear_schedule_with_warmup(optimizer, int(args.warmup_ratio * num_train_steps), num_train_steps)
    item_table = item_table.to(device, non_blocking=True)
    # 5) O 段训练（LLaRA LM 损失）
    if is_main:
        print(f"[Train] O-stage (LLaRA) epochs={args.epochs}")
    scaler = torch.amp.GradScaler('cuda', enabled=args.fp16 and torch.cuda.is_available())
    for ep in range(1, args.epochs + 1):
        if is_distributed and train_sampler is not None:
            train_sampler.set_epoch(ep)
        model.train(); sr2llm.train()
        total=0.0; nstep=0
        for batch in tqdm(train_loader, desc="Train(O)-LLaRA", disable=not is_main):
            loss = lm_loss_llara(model, tok, sr2llm, item_table, batch, device, fp16=args.fp16)
            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(params, args.grad_clip)

            scaler.step(optimizer);
            scaler.update()
            if scheduler is not None: scheduler.step()
            total += float(loss.detach().item());
            nstep += 1

        if is_main:
            print(f"[Train][O] epoch={ep} loss={total/max(1,nstep):.4f}")

    if is_distributed:
        barrier_gloo()
    if is_main:
        k_list = sorted(set(args.topk))
        mO = evaluate_llara_dataset(model, tok, sr2llm, item_table.to(device), ld_O, idx2title, k_list, device,
                                    fp16=args.fp16, is_main=True, max_eval=args.max_eval_eval,
                                    cand_chunk_size=args.eval_chunk_size)
        mT = evaluate_llara_dataset(model, tok, sr2llm, item_table.to(device), ld_T, idx2title, k_list, device,
                                    fp16=args.fp16, is_main=True, max_eval=args.max_eval_eval,
                                    cand_chunk_size=args.eval_chunk_size)
        with open(os.path.join(args.output_dir, "metrics_original.json"), "w", encoding="utf-8") as f:
            json.dump(mO, f, ensure_ascii=False, indent=2)
        with open(os.path.join(args.output_dir, "metrics_test.json"), "w", encoding="utf-8") as f:
            json.dump(mT, f, ensure_ascii=False, indent=2)

    # 保存 default 适配器 + 干净 base（含 <BEH>） + SR2LLM + mid2idx
    if is_main and args.use_lora:
        default_dir = os.path.join(args.output_dir, "default")
        ensure_dir(default_dir)
        peft_wrapped = model.module if hasattr(model, "module") else model
        peft_wrapped.save_pretrained(default_dir, selected_adapters=["default"])
        tok.save_pretrained(default_dir)

        base_dir = os.path.join(args.output_dir, "base_with_new_tokens")
        save_clean_base_with_beh_token(
            peft_wrapped=peft_wrapped,
            tokenizer=tok,
            out_dir=base_dir,
            orig_base_path=args.model_name_or_path
        )
        print(f"[SAVE] clean base saved to {base_dir}")
        torch.save((sr2llm.module if hasattr(sr2llm,"module") else sr2llm).state_dict(),
                   os.path.join(args.output_dir, "sr2llm_default.pt"))
        print(f"[SAVE] SR2LLM -> {os.path.join(args.output_dir, 'sr2llm_default.pt')}")

    if is_distributed:
        barrier_gloo()

    try:
        del optimizer, scheduler, scaler
    except Exception:
        pass
    try:
        peft_wrapped = locals().get("peft_wrapped", None)
        _free_cuda(model, peft_wrapped, sr2llm, sas)
    except Exception:
        pass
    aggressive_gc()
    if is_distributed:
        barrier_gloo()


def fresh_lora_model_from_base(base_dir, device, lora_r, lora_alpha, lora_dropout, load_dtype):
    tok2 = AutoTokenizer.from_pretrained(base_dir, use_fast=True)
    if tok2.pad_token is None: tok2.pad_token = tok2.eos_token
    m = AutoModelForCausalLM.from_pretrained(
        base_dir,
        low_cpu_mem_usage=True,
        dtype=load_dtype,
        attn_implementation="sdpa"  # 同上，如需可改 "sdpa"
    )
    if hasattr(m.config, "use_cache"): m.config.use_cache = False
    lcfg = LoraConfig(
        r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
        bias="none", task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
    )
    m = get_peft_model(m, lcfg)
    m.to(device)
    return tok2, m


def _load_item_table_and_sr2llm(args, device, d_sas, llm_hidden, is_main):
    item_path = os.path.join(args.output_dir, "sasrec_item_table.pt")
    sr2_path  = os.path.join(args.output_dir, "sr2llm_default.pt")
    if not os.path.isfile(item_path):
        raise FileNotFoundError(f"Missing SASRec item table: {item_path} (请先跑 pre)")
    item_table = torch.load(item_path, map_location='cpu')
    assert item_table.shape[1] == d_sas, f"SASRec dim mismatch: {item_table.shape[1]} vs {d_sas}"

    sr2llm = nn.Sequential(nn.Linear(d_sas, llm_hidden), nn.GELU(), nn.Linear(llm_hidden, llm_hidden)).to(device)
    if os.path.isfile(sr2_path):
        sd = torch.load(sr2_path, map_location='cpu')
        sr2llm.load_state_dict(sd, strict=False)
    else:
        if is_main:
            print(f"[WARN] SR2LLM not found at {sr2_path}, use fresh init.")
    return item_table, sr2llm


def stage_lora(args, device, is_distributed, local_rank, is_main, load_dtype):
    base_dir = args.resume_base_dir or os.path.join(args.output_dir, "base_with_new_tokens")
    if not os.path.isdir(base_dir):
        raise FileNotFoundError(f"[stage lora] base_with_new_tokens not found at {base_dir}")
    default_dir = os.path.join(args.output_dir, "default")
    if not os.path.isdir(default_dir):
        raise FileNotFoundError(f"[stage lora] default adapter dir not found at {default_dir}")

    tok_lora, lora_model = fresh_lora_model_from_base(
        base_dir, device, args.lora_r, args.lora_alpha, args.lora_dropout, load_dtype
    )
    lora_model.load_adapter(default_dir, adapter_name="default", is_trainable=True)
    lora_model.set_adapter("default")

    # 构建 mid2idx、数据、title
    mid2idx, n_items = build_or_load_mid2idx(args, is_main=is_main)
    idx2title = build_idx2title(n_items)
    ds_trainO, ds_O, ds_T, ds_F = make_llara_datasets_for_stage(args, tok_lora, mid2idx, n_items, idx2title)
    pad_id = tok_lora.pad_token_id
    collate = lambda batch: llara_collate(batch, pad_id)

    # SR2LLM & item_table
    d_llm = (lora_model.module if hasattr(lora_model, "module") else lora_model).get_input_embeddings().weight.shape[1]
    item_table, sr2llm = _load_item_table_and_sr2llm(args, device, args.d_sas, d_llm, is_main=is_main)

    if is_distributed:
        lora_model = torch.nn.parallel.DistributedDataParallel(
            lora_model, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None, find_unused_parameters=False
        )
        sr2llm = torch.nn.parallel.DistributedDataParallel(
            sr2llm, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None, find_unused_parameters=False
        )

    ft_sampler = DistributedSampler(ds_F, num_replicas=(dist.get_world_size() if is_distributed else 1),
                                    rank=(dist.get_rank() if is_distributed else 0), shuffle=True) if is_distributed else None
    ld_F_ddp = DataLoader(ds_F, batch_size=args.finetune_batch_size,
                          shuffle=(ft_sampler is None), sampler=ft_sampler,
                          num_workers=2, pin_memory=True, collate_fn=collate)

    # 优化器 = LoRA + SR2LLM
    FT_optim = torch.optim.AdamW(list(lora_model.parameters())+list(sr2llm.parameters()), lr=args.lr)
    FT_sched = get_linear_schedule_with_warmup(FT_optim, int(args.warmup_ratio*len(ld_F_ddp)*max(1,args.finetune_epochs)),
                                               len(ld_F_ddp)*max(1,args.finetune_epochs))

    for ep in range(1, args.finetune_epochs + 1):
        if is_distributed and ft_sampler is not None: ft_sampler.set_epoch(ep)
        loss = finetune_one_epoch_llara(
            lora_model, tok_lora, sr2llm, item_table, ld_F_ddp, device,
            optimizer=FT_optim, scheduler=FT_sched, fp16=args.fp16,
            plugin="none", replay_buf=None,
            replay_ratio=args.replay_ratio, grad_clip=1.0, log_every=100
        )
        if is_main: print(f"[Method: LoRA] F epoch={ep} loss={loss:.4f}")

    if is_main:
        base_eval_model = lora_model.module if hasattr(lora_model, "module") else lora_model
        k_list = sorted(set(args.topk))
        mO = evaluate_llara_dataset(base_eval_model, tok_lora, (sr2llm.module if hasattr(sr2llm,"module") else sr2llm),
                                    item_table.to(device), ds_O, idx2title, k_list, device, fp16=args.fp16, is_main=True,
                                    max_eval=args.max_eval_eval, cand_chunk_size=args.eval_chunk_size)
        mT = evaluate_llara_dataset(base_eval_model, tok_lora, (sr2llm.module if hasattr(sr2llm,"module") else sr2llm),
                                    item_table.to(device), ds_T, idx2title, k_list, device, fp16=args.fp16, is_main=True,
                                    max_eval=args.max_eval_eval, cand_chunk_size=args.eval_chunk_size)
        with open(os.path.join(args.output_dir, "metrics_original_lora.json"), "w", encoding="utf-8") as f: json.dump(mO, f, ensure_ascii=False, indent=2)
        with open(os.path.join(args.output_dir, "metrics_test_lora.json"), "w", encoding="utf-8") as f: json.dump(mT, f, ensure_ascii=False, indent=2)
        save_lora = os.path.join(args.output_dir, "lora_adapter_lora"); ensure_dir(save_lora); base_eval_model.save_pretrained(save_lora)
        # 保存微调后的 SR2LLM（覆盖/另存）
        torch.save((sr2llm.module if hasattr(sr2llm,"module") else sr2llm).state_dict(),
                   os.path.join(args.output_dir, "sr2llm_lora.pt"))
        print("[Done] LoRA:", mO, mT)

    try:
        del FT_optim, FT_sched
    except Exception:
        pass
    _free_cuda(lora_model, sr2llm)

    if is_distributed: barrier_gloo()


def stage_replay(args, device, is_distributed, local_rank, is_main, load_dtype):
    base_dir = args.resume_base_dir or os.path.join(args.output_dir, "base_with_new_tokens")
    if not os.path.isdir(base_dir):
        raise FileNotFoundError(f"[stage replay] base_with_new_tokens not found at {base_dir}")
    default_dir = os.path.join(args.output_dir, "default")
    if not os.path.isdir(default_dir):
        raise FileNotFoundError(f"[stage replay] default adapter dir not found at {default_dir}")

    tok_rep, rep_model = fresh_lora_model_from_base(
        base_dir, device, args.lora_r, args.lora_alpha, args.lora_dropout, load_dtype
    )
    rep_model.load_adapter(default_dir, adapter_name="default", is_trainable=True)
    rep_model.set_adapter("default")

    mid2idx, n_items = build_or_load_mid2idx(args, is_main=is_main)
    idx2title = build_idx2title(n_items)
    ds_trainO, ds_O, ds_T, ds_F = make_llara_datasets_for_stage(args, tok_rep, mid2idx, n_items, idx2title)
    pad_id = tok_rep.pad_token_id
    collate = lambda batch: llara_collate(batch, pad_id)

    d_llm = (rep_model.module if hasattr(rep_model, "module") else rep_model).get_input_embeddings().weight.shape[1]
    item_table, sr2llm = _load_item_table_and_sr2llm(args, device, args.d_sas, d_llm, is_main=is_main)

    if is_distributed:
        rep_model = torch.nn.parallel.DistributedDataParallel(
            rep_model, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None, find_unused_parameters=False
        )
        sr2llm = torch.nn.parallel.DistributedDataParallel(
            sr2llm, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None, find_unused_parameters=False
        )

    ft_sampler = DistributedSampler(ds_F, num_replicas=(dist.get_world_size() if is_distributed else 1),
                                    rank=(dist.get_rank() if is_distributed else 0), shuffle=True) if is_distributed else None
    ld_F_ddp = DataLoader(ds_F, batch_size=args.finetune_batch_size,
                          shuffle=(ft_sampler is None), sampler=ft_sampler,
                          num_workers=2, pin_memory=True, collate_fn=collate)

    rb_sampler = DistributedSampler(ds_O, num_replicas=(dist.get_world_size() if is_distributed else 1),
                                    rank=(dist.get_rank() if is_distributed else 0), shuffle=True) if is_distributed else None
    replay_buf = ReplayBuffer(ds_O, collate, batch_size=args.finetune_batch_size, sampler=rb_sampler)

    FT_optim = torch.optim.AdamW(list(rep_model.parameters())+list(sr2llm.parameters()), lr=args.lr)
    FT_sched = get_linear_schedule_with_warmup(FT_optim, int(args.warmup_ratio*len(ld_F_ddp)*max(1,args.finetune_epochs)),
                                               len(ld_F_ddp)*max(1,args.finetune_epochs))

    for ep in range(1, args.finetune_epochs + 1):
        if is_distributed and ft_sampler is not None: ft_sampler.set_epoch(ep)
        if is_distributed: replay_buf.set_epoch(ep)
        loss = finetune_one_epoch_llara(
            rep_model, tok_rep, sr2llm, item_table, ld_F_ddp, device,
            optimizer=FT_optim, scheduler=FT_sched, fp16=args.fp16,
            plugin="replay", replay_buf=replay_buf,
            replay_ratio=args.replay_ratio, grad_clip=1.0, log_every=100
        )
        if is_main: print(f"[Method: LoRA+Replay] F epoch={ep} loss={loss:.4f}")

    if is_main:
        base_eval_model = rep_model.module if hasattr(rep_model, "module") else rep_model
        k_list = sorted(set(args.topk))
        mO = evaluate_llara_dataset(base_eval_model, tok_rep, (sr2llm.module if hasattr(sr2llm,"module") else sr2llm),
                                    item_table.to(device), ds_O, idx2title, k_list, device, fp16=args.fp16, is_main=True,
                                    max_eval=args.max_eval_eval, cand_chunk_size=args.eval_chunk_size)
        mT = evaluate_llara_dataset(base_eval_model, tok_rep, (sr2llm.module if hasattr(sr2llm,"module") else sr2llm),
                                    item_table.to(device), ds_T, idx2title, k_list, device, fp16=args.fp16, is_main=True,
                                    max_eval=args.max_eval_eval, cand_chunk_size=args.eval_chunk_size)
        with open(os.path.join(args.output_dir, "metrics_original_lora_replay.json"), "w", encoding="utf-8") as f: json.dump(mO, f, ensure_ascii=False, indent=2)
        with open(os.path.join(args.output_dir, "metrics_test_lora_replay.json"), "w", encoding="utf-8") as f: json.dump(mT, f, ensure_ascii=False, indent=2)
        save_lora = os.path.join(args.output_dir, "lora_adapter_lora_replay"); ensure_dir(save_lora); base_eval_model.save_pretrained(save_lora)
        print("[Done] LoRA+Replay:", mO, mT)

    try:
        del FT_optim, FT_sched, replay_buf
    except Exception:
        pass
    _free_cuda(rep_model, sr2llm)
    if is_distributed: barrier_gloo()


def stage_raie(args, device, is_distributed, local_rank, is_main, load_dtype):
    """
    真·RAIE：KMeans 划分 item 空间 -> 每区一套 LoRA -> 按历史路由训练/评测。
    SR2LLM 先采用“共享”一套，简化实现且稳定。
    """
    base_dir = args.resume_base_dir or os.path.join(args.output_dir, "base_with_new_tokens")
    if not os.path.isdir(base_dir):
        raise FileNotFoundError(f"[stage raie] base_with_new_tokens not found at {base_dir}")

    # 1) 起“干净 base + 可训练 LoRA”
    tok, peft_model = fresh_lora_model_from_base(
        base_dir, device, args.lora_r, args.lora_alpha, args.lora_dropout, load_dtype
    )
    # 加载 O 段默认适配器作为初始化（冻结或不加载都行；这里不强制要求）
    default_dir = os.path.join(args.output_dir, "default")
    if not os.path.isdir(default_dir):
        raise FileNotFoundError(f"[stage raie] default adapter dir not found at {default_dir}")
    # 先加载 default，作为底座参数初始化（可不训练）
    peft_model.load_adapter(default_dir, adapter_name="default", is_trainable=False)
    peft_model.set_adapter("default")

    # 2) 数据与标题映射
    mid2idx, n_items = build_or_load_mid2idx(args, is_main=is_main)
    idx2title = build_idx2title(n_items)
    ds_trainO, ds_O, ds_T, ds_F = make_llara_datasets_for_stage(args, tok, mid2idx, n_items, idx2title)

    # 3) 载入 item_table & 共享 SR2LLM
    d_llm = (peft_model.module if hasattr(peft_model, "module") else peft_model).get_input_embeddings().weight.shape[1]
    item_table, sr2llm = _load_item_table_and_sr2llm(args, device, args.d_sas, d_llm, is_main=is_main)
    item_table = item_table.to(device, non_blocking=True)

    # 4) KMeans 分区（在 CPU 上完成）
    if is_main:
        print(f"[RAIE] KMeans clustering: k={args.raie_k}")
    item2region_np, centers_cpu = build_item_regions(item_table, k=args.raie_k, seed=args.seed)  # centers: cpu
    # 说明：item2region 暂未直接用，但可用于其它路由策略；本实现按“历史均值”路由

    # 5) 按区域切 F 段数据
    subsets_F = split_dataset_by_region(ds_F, item_table, centers_cpu)  # List[Subset], len=k
    # 可选：也把 O 段切分做 replay（此处先不做 replay，想做很简单，可照 stage_replay 的思路）

    region_adapter_names = [f"region_{i}" for i in range(args.raie_k)]
    base_plain = peft_model  # 未包 DDP
    for rid in range(args.raie_k):
        try:
            base_plain.delete_adapter(region_adapter_names[rid])
        except Exception:
            pass
        lcfg = LoraConfig(
            r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
            bias="none", task_type=TaskType.CAUSAL_LM,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
        base_plain.add_adapter(region_adapter_names[rid], lcfg)
    base_plain.set_adapter("default")

    # 6) DDP 包一下（只包 peft_model 和 sr2llm）
    if is_distributed:
        peft_model = torch.nn.parallel.DistributedDataParallel(
            peft_model, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None, find_unused_parameters=False
        )
        sr2llm = torch.nn.parallel.DistributedDataParallel(
            sr2llm, device_ids=[local_rank] if torch.cuda.is_available() else None,
            output_device=local_rank if torch.cuda.is_available() else None, find_unused_parameters=False
        )

    # 7) 为每个区域创建并训练适配器
    region_adapter_names = [f"region_{i}" for i in range(args.raie_k)]
    for rid in range(args.raie_k):
        # 构造 DataLoader（只含该区域）
        if len(subsets_F[rid]) == 0:
            if is_main:
                print(f"[RAIE] region {rid} has 0 samples in F; skip training.")
            continue

        sampler = DistributedSampler(subsets_F[rid],
                                     num_replicas=(dist.get_world_size() if is_distributed else 1),
                                     rank=(dist.get_rank() if is_distributed else 0),
                                     shuffle=True) if is_distributed else None
        loader = DataLoader(subsets_F[rid],
                            batch_size=args.finetune_batch_size,
                            shuffle=(sampler is None),
                            sampler=sampler,
                            num_workers=2, pin_memory=True,
                            collate_fn=lambda b: llara_collate(b, tok.pad_token_id))

        base = peft_model.module if hasattr(peft_model, "module") else peft_model

        lcfg = LoraConfig(
            r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
            bias="none", task_type=TaskType.CAUSAL_LM,
            target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
        )
        base.set_adapter(region_adapter_names[rid])

        # 优化器 = 当前 LoRA + 共享 SR2LLM
        optim = torch.optim.AdamW(list(peft_model.parameters()) + list(sr2llm.parameters()), lr=args.lr)
        sched = get_linear_schedule_with_warmup(optim,
                                                int(args.warmup_ratio*len(loader)*max(1,args.finetune_epochs)),
                                                len(loader)*max(1,args.finetune_epochs))

        for ep in range(1, args.finetune_epochs+1):
            if is_distributed and isinstance(sampler, DistributedSampler):
                sampler.set_epoch(ep)
            loss = finetune_one_epoch_llara(
                peft_model, tok, sr2llm, item_table, loader, device,
                optimizer=optim, scheduler=sched, fp16=args.fp16,
                plugin="none", replay_buf=None,
                replay_ratio=0.0, grad_clip=1.0, log_every=100
            )
            if is_main:
                print(f"[RAIE][region {rid}] epoch={ep} loss={loss:.4f}")

        # 保存该区域的 LoRA
        if is_main:
            save_dir = os.path.join(args.output_dir, f"raie_adapter_region_{rid}")
            ensure_dir(save_dir)
            base.save_pretrained(save_dir, selected_adapters=[region_adapter_names[rid]])
            print(f"[RAIE] saved adapter for region {rid} -> {save_dir}")

        # 清理优化器，释放显存
        try:
            del optim, sched
        except Exception:
            pass
        aggressive_gc()

    if is_distributed:
        barrier_gloo()

    # 8) 训练结束 -> 评测 RAIE（按历史路由动态切 adapter）
    if is_main:
        # 先把所有区域 LoRA 重新 load 回来（稳妥起见）
        base = peft_model.module if hasattr(peft_model, "module") else peft_model
        loaded_names = []
        for rid in range(args.raie_k):
            ad_dir = os.path.join(args.output_dir, f"raie_adapter_region_{rid}")
            if os.path.isdir(ad_dir):
                base.load_adapter(ad_dir, adapter_name=region_adapter_names[rid], is_trainable=False)
                loaded_names.append(region_adapter_names[rid])

        if not loaded_names:
            print("[RAIE] no region adapters loaded, skip eval.")
            return

        k_list = sorted(set(args.topk))
        mO = evaluate_llara_raie(peft_model, loaded_names, tok, (sr2llm.module if hasattr(sr2llm,"module") else sr2llm),
                                 item_table, ds_O, idx2title, k_list, device, centers_cpu,
                                 fp16=args.fp16, is_main=True, max_eval=args.max_eval_eval,
                                 cand_chunk_size=args.eval_chunk_size)
        mT = evaluate_llara_raie(peft_model, loaded_names, tok, (sr2llm.module if hasattr(sr2llm,"module") else sr2llm),
                                 item_table, ds_T, idx2title, k_list, device, centers_cpu,
                                 fp16=args.fp16, is_main=True, max_eval=args.max_eval_eval,
                                 cand_chunk_size=args.eval_chunk_size)
        with open(os.path.join(args.output_dir, "metrics_original_raie.json"), "w", encoding="utf-8") as f: json.dump(mO, f, ensure_ascii=False, indent=2)
        with open(os.path.join(args.output_dir, "metrics_test_raie.json"), "w", encoding="utf-8") as f: json.dump(mT, f, ensure_ascii=False, indent=2)
        print("[Done] RAIE:", mO, mT)

    _free_cuda(peft_model, sr2llm)
    if is_distributed: barrier_gloo()

# ==============================
# ============ main ============
# ==============================
def main():
    ap = argparse.ArgumentParser()
    # 模型 & 数据路径
    ap.add_argument('--model_name_or_path', type=str, default='/home/zj/model/Llama-2-7b-hf')
    ap.add_argument('--data_dir', type=str, default='/home/zj/code/Amazon_electronics/')
    ap.add_argument('--output_dir', type=str, default='./runs/llara_Amazon_electronics')

    ap.add_argument('--train_jsonl_path', type=str, default='')
    ap.add_argument('--original_jsonl_path', type=str, default='')
    ap.add_argument('--test_jsonl_path', type=str, default='')
    ap.add_argument('--item_ids_path', type=str, default='')
    ap.add_argument('--finetune_jsonl_path', type=str, default='')
    ap.add_argument('--eval_prefilter', type=int, default=10,
                    help='prefilter top-M candidates per example before LLM ranking')
    ap.add_argument('--eval_chunk_size', type=int, default=2,
                    help='candidate scoring batch size per prompt during evaluation')

    # 新增：阶段控制
    ap.add_argument('--stage', type=str, choices=['pre','lora','replay','raie','all'], default='pre',
                    help='选择运行阶段')
    ap.add_argument('--resume_base_dir', type=str, default='',
                    help='已保存的 base_with_new_tokens 路径，不填则使用 output_dir/base_with_new_tokens')
    ap.add_argument('--skip_pre_if_exists', action='store_true', default=True,
                    help='当 stage=all 且已存在 base_with_new_tokens 时，跳过 pre（默认开启）')

    # 索引构建（新增 collaborative）
    ap.add_argument('--item_indexing', type=str, choices=['sequential', 'random', 'collaborative'], default='sequential')
    ap.add_argument('--min_user_len', type=int, default=5)
    ap.add_argument('--max_history', type=int, default=10)

    # SASRec
    ap.add_argument('--d_sas', type=int, default=32)
    ap.add_argument('--epochs_sasrec', type=int, default=1)
    ap.add_argument('--lr_sas', type=float, default=1e-3)

    # 训练（LLaRA O 段）
    ap.add_argument('--epochs', type=int, default=5)
    ap.add_argument('--batch_size', type=int, default=10)
    ap.add_argument('--lr', type=float, default=2e-4)
    ap.add_argument('--warmup_ratio', type=float, default=0.05)
    ap.add_argument('--weight_decay', type=float, default=0.01)
    ap.add_argument('--fp16', action='store_true', default=True)
    ap.add_argument('--seed', type=int, default=42)
    ap.add_argument('--grad_clip', type=float, default=1.0)
    ap.add_argument('--max_eval_eval', type=int, default=10)  # 评测上限（-1 全量）

    # LoRA
    ap.add_argument('--use_lora', action='store_true', default=True)
    ap.add_argument('--lora_r', type=int, default=8)
    ap.add_argument('--lora_alpha', type=int, default=16)
    ap.add_argument('--lora_dropout', type=float, default=0.05)
    ap.add_argument('--grad_checkpointing', action='store_true')

    ap.add_argument('--raie_k', type=int, default=3, help='RAIE KMeans 分区数量（区域数）')
    ap.add_argument('--raie_route', type=str, default='hist-mean',
                    choices=['hist-mean'], help='RAIE 路由策略（当前实现 hist-mean）')


    # 评测
    ap.add_argument('--topk', type=int, nargs='+', default=[5, 10, 20])

    # F 段方法公共
    ap.add_argument('--finetune_batch_size', type=int, default=32)
    ap.add_argument('--finetune_epochs', type=int, default=3)

    # Replay
    ap.add_argument('--replay_ratio', type=float, default=0.3)

    args = ap.parse_args()

    # DDP init
    is_distributed, rank, world_size, local_rank = init_distributed()
    is_main = (rank == 0)
    set_seed(args.seed, rank)

    # dtype 统一控制
    load_dtype = torch.float16 if (args.fp16 and torch.cuda.is_available()) else torch.float32

    if torch.cuda.is_available():
        device = torch.device(f'cuda:{local_rank}' if is_distributed else 'cuda')
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.set_float32_matmul_precision("high")
    else:
        device = torch.device('cpu')

    if is_main:
        ensure_dir(args.output_dir)

    # 分阶段执行
    if args.stage in ("all", "pre"):
        need_pre = True
        if args.stage == "all" and args.skip_pre_if_exists:
            base_dir_default = args.resume_base_dir or os.path.join(args.output_dir, "base_with_new_tokens")
            if os.path.isdir(base_dir_default):
                need_pre = False
                if is_main:
                    print(f"[SKIP] Detected existing base at {base_dir_default}, skip pre-stage as requested.")
        if need_pre:
            stage_pre(args, device, is_distributed, local_rank, is_main, load_dtype)
        else:
            if is_distributed: barrier_gloo()

    if args.stage in ("all", "lora"):
        stage_lora(args, device, is_distributed, local_rank, is_main, load_dtype)

    if args.stage in ("all", "replay"):
        stage_replay(args, device, is_distributed, local_rank, is_main, load_dtype)

    if args.stage in ("all", "raie"):
        stage_raie(args, device, is_distributed, local_rank, is_main, load_dtype)

    if is_main:
        print("\n=== SELECTED STAGE(S) FINISHED ===")
        print("Results saved in:", args.output_dir)

    aggressive_gc()
    cleanup_distributed()


if __name__ == '__main__':
    main()
