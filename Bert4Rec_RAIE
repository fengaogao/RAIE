# raie_bert4rec_five_modes.py
# -*- coding: utf-8 -*-
import os, math, json, argparse
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from collections import defaultdict

import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset
from torch.nn.utils import clip_grad_norm_

from transformers import BertConfig, BertForMaskedLM, get_linear_schedule_with_warmup

# ---- NEW: PEFT / LoRA ----
try:
    from peft import LoraConfig, get_peft_model, TaskType
    PEFT_AVAILABLE = True
except Exception as e:
    PEFT_AVAILABLE = False
    _PEFT_ERR = e

# ------------------------------
# Utilities
# ------------------------------
def set_seed(seed: int = 42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def read_jsonl(path: str) -> List[dict]:
    rows = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

def load_item_vocab(data_dir: str):
    """Build token <-> id mapping from item_ids.json.
    Special tokens come first.
    Returns:
      token2id: dict
      id2token: list
      item_token_ids: List[int]  # ids of the real item tokens (exclude specials)
    """
    item_json = os.path.join(data_dir, 'item_ids.json')
    with open(item_json, 'r', encoding='utf-8') as f:
        obj = json.load(f)
    item_ids: List[int] = obj['item_ids']

    specials = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']
    token2id = {tok: i for i, tok in enumerate(specials)}
    id2token = specials.copy()

    for mid in sorted(item_ids):
        tok = f"<item_{mid}>"
        token2id[tok] = len(id2token)
        id2token.append(tok)

    item_token_ids = list(range(len(specials), len(id2token)))
    return token2id, id2token, item_token_ids

# ------------------------------
# Dataset & Collators
# ------------------------------
@dataclass
class Example:
    prompt_tokens: List[str]
    target_token: str

@dataclass
class StreamExample:
    items: List[str]  # only sequence, no target

class StreamDataset(Dataset):
    """
    original_stream.jsonl:
    { "user_id": ..., "items": "<item_1> <item_2> ...", "timestamps": "..." (ignored) }
    """
    def __init__(self, jsonl_path: str):
        rows = read_jsonl(jsonl_path)
        self.examples: List[StreamExample] = []
        for r in rows:
            s = (r.get('items') or "").strip()
            if not s:
                continue
            toks = s.split()
            if len(toks) == 0:
                continue
            self.examples.append(StreamExample(toks))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx: int) -> StreamExample:
        return self.examples[idx]

class NextItemDataset(Dataset):
    def __init__(self, jsonl_path: str):
        rows = read_jsonl(jsonl_path)
        self.examples: List[Example] = []
        for r in rows:
            prompt = r['prompt'].strip()
            target = r['target'].strip()
            if not prompt or not target:
                continue
            self.examples.append(Example(prompt.split(), target))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx: int) -> Example:
        return self.examples[idx]

class ClozeTrainCollator:
    def __init__(self, token2id: Dict[str, int], max_len: int, pad_id: int, mask_id: int,
                 cls_id: int, mask_prob: float = 0.15):
        self.tok2id = token2id
        self.max_len = max_len
        self.pad_id = pad_id
        self.mask_id = mask_id
        self.cls_id = cls_id
        self.vocab_size = len(token2id)
        self.mask_prob = mask_prob

    def _enc(self, t: str) -> int:
        return self.tok2id.get(t, self.tok2id['[UNK]'])

    def __call__(self, batch: List[StreamExample]):
        B = len(batch)
        L = self.max_len
        seq_len = L + 1             # + [CLS]

        input_ids = torch.full((B, seq_len), self.pad_id, dtype=torch.long)
        attention_mask = torch.zeros((B, seq_len), dtype=torch.long)
        labels = torch.full((B, seq_len), -100, dtype=torch.long)  # supervise only masked positions

        for i, ex in enumerate(batch):
            ids = [self._enc(t) for t in ex.items[-self.max_len:]]
            real_len = len(ids)
            if real_len == 0:
                continue

            # choose mask positions
            n_mask = max(1, int(math.ceil(real_len * self.mask_prob)))
            cand_idx = np.arange(real_len)
            mask_pos = np.random.choice(cand_idx, size=n_mask, replace=False)

            # 80/10/10 strategy
            seq = ids[:]
            for p in mask_pos:
                r = np.random.rand()
                if r < 0.8:
                    seq[p] = self.mask_id
                elif r < 0.9:
                    seq[p] = np.random.randint(0, self.vocab_size)
                else:
                    pass  # keep original

            pos = 0
            input_ids[i, pos] = self.cls_id; pos += 1
            input_ids[i, pos:pos+real_len] = torch.tensor(seq, dtype=torch.long)
            attention_mask[i, :pos+real_len] = 1

            # label aligns with [CLS] offset 1
            for p in mask_pos:
                labels[i, 1 + p] = ids[p]

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
        }

class EvalCollator:
    """
    Evaluation / next-item training:
    [CLS] + prompt[-max_len:] + [MASK], label = target_id at the final position.
    """
    def __init__(
        self,
        token2id: Dict[str, int],
        max_len: int,
        mask_id: int,
        cls_id: int,
        pad_id: int,
        do_prompt_mask: bool = False,
        prompt_mask_prob: float = 0.15,
    ):
        self.tok2id = token2id
        self.max_len = max_len
        self.mask_id = mask_id
        self.cls_id = cls_id
        self.pad_id = pad_id
        self.do_prompt_mask = do_prompt_mask
        self.prompt_mask_prob = prompt_mask_prob
        self.vocab_size = len(token2id)

    def encode_token(self, t: str) -> int:
        return self.tok2id.get(t, self.tok2id['[UNK]'])

    def __call__(self, batch: List[Example]):
        B = len(batch)
        seq_len = self.max_len + 2  # [CLS] + max_len prompt + [MASK]

        input_ids = torch.full((B, seq_len), self.pad_id, dtype=torch.long)
        attention_mask = torch.zeros((B, seq_len), dtype=torch.long)
        labels = torch.full((B,), -100, dtype=torch.long)

        for i, ex in enumerate(batch):
            prompt_ids = [self.encode_token(t) for t in ex.prompt_tokens[-self.max_len:]]
            tgt_id = self.encode_token(ex.target_token)

            pos = 0
            input_ids[i, pos] = self.cls_id; pos += 1

            if self.do_prompt_mask and self.prompt_mask_prob > 0:
                masked_prompt = []
                for pid in prompt_ids:
                    if np.random.rand() < self.prompt_mask_prob:
                        p = np.random.rand()
                        if p < 0.8:
                            masked_prompt.append(self.mask_id)
                        elif p < 0.9:
                            masked_prompt.append(np.random.randint(0, self.vocab_size))
                        else:
                            masked_prompt.append(pid)
                    else:
                        masked_prompt.append(pid)
                prompt_ids = masked_prompt

            Lp = len(prompt_ids)
            input_ids[i, pos:pos+Lp] = torch.tensor(prompt_ids, dtype=torch.long)
            pos += Lp

            input_ids[i, pos] = self.mask_id
            attention_mask[i, :pos+1] = 1
            labels[i] = tgt_id

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
        }

# ------------------------------
# Metrics
# ------------------------------
def recall_ndcg_at_k(
    logits: torch.Tensor,
    labels: torch.Tensor,
    item_token_ids: List[int],
    topk: Tuple[int, ...] = (5, 10, 20),
) -> Dict[str, torch.Tensor]:
    device = logits.device
    V = logits.shape[1]

    # restrict to item vocab
    mask = torch.full((V,), float('-inf'), device=device)
    mask[item_token_ids] = 0.0
    logits = logits + mask

    max_k = max(topk)
    _, topk_idx = torch.topk(logits, k=max_k, dim=1)

    B = labels.numel()
    labels_exp = labels.view(-1, 1).expand(-1, max_k)
    match = (topk_idx == labels_exp).float()  # [B, max_k]

    out_hit = {}
    out_dcg = {}
    for K in topk:
        mK = match[:, :K]
        hit = mK.max(dim=1).values
        pos = torch.argmax(mK, dim=1)
        dcg = hit * (1.0 / torch.log2(pos.float() + 2.0))
        out_hit[K] = hit.sum()
        out_dcg[K] = dcg.sum()
    return {'hit': out_hit, 'dcg': out_dcg, 'n': torch.tensor(B, device=device)}

# ------------------------------
# Training & Evaluation (O-stage, Eval)
# ------------------------------
def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler,
    device: torch.device,
    item_token_ids: List[int],
    objective: str,
    grad_clip: float = 1.0,
) -> Dict[str, float]:
    model.train()
    total_loss = 0.0
    n_steps = 0

    for batch in tqdm(loader, desc='Train', leave=False):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        optimizer.zero_grad()

        if objective == 'cloze':
            labels = batch['labels'].to(device)  # [B, L]
            out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = out.loss
        else:  # 'next'
            labels = batch['labels'].to(device)  # [B]
            out = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = out.logits  # [B, L, V]
            final_pos = attention_mask.sum(dim=1) - 1
            rows = torch.arange(input_ids.size(0), device=device)
            logits_final = logits[rows, final_pos, :]

            V = logits_final.size(-1)
            mask_vec = torch.full((V,), float('-inf'), device=device)
            mask_vec[item_token_ids] = 0.0
            logits_final = logits_final + mask_vec
            loss = nn.functional.cross_entropy(logits_final, labels)

        loss.backward()
        if grad_clip is not None and grad_clip > 0:
            clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()
        if scheduler is not None:
            scheduler.step()

        total_loss += loss.item()
        n_steps += 1

    return {"train_loss": total_loss / max(n_steps, 1)}

@torch.no_grad()
def evaluate(
    model: nn.Module,
    loader: DataLoader,
    device: torch.device,
    item_token_ids: List[int],
    topk: Tuple[int, ...] = (5, 10, 20),
) -> Dict[str, float]:
    model.eval()
    sum_hit = {K: 0.0 for K in topk}
    sum_dcg = {K: 0.0 for K in topk}
    N = 0

    with torch.no_grad():
        for batch in tqdm(loader, desc='Eval', leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            out = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = out.logits  # [B, L, V]
            final_pos = attention_mask.sum(dim=1) - 1
            rows = torch.arange(input_ids.size(0), device=device)
            logits_final = logits[rows, final_pos, :]

            m = recall_ndcg_at_k(logits_final, labels, item_token_ids, topk)
            for K in topk:
                sum_hit[K] += float(m['hit'][K].item())
                sum_dcg[K] += float(m['dcg'][K].item())
            N += int(m['n'].item())

    out = {}
    for K in topk:
        out[f'Recall@{K}'] = sum_hit[K] / max(N, 1)
        out[f'NDCG@{K}'] = sum_dcg[K] / max(N, 1)
    return out

# ------------------------------
# Plugins: Replay / EWC / LwF
# ------------------------------
class ReplayBuffer:
    def __init__(self, base_dataset: Dataset, collate_fn, batch_size: int):
        self.loader = DataLoader(base_dataset, batch_size=batch_size, shuffle=True,
                                 num_workers=0, collate_fn=collate_fn)
        self.it = iter(self.loader)
    def next_batch(self):
        try:
            return next(self.it)
        except StopIteration:
            self.it = iter(self.loader)
            return next(self.it)

@torch.no_grad()
def _clone_params(model: nn.Module):
    return {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}

def compute_fisher_diag_bert(model: BertForMaskedLM, loader: DataLoader, device, max_batches: int = 100):
    model.eval()
    fisher = {n: torch.zeros_like(p, device=device) for n,p in model.named_parameters() if p.requires_grad}
    n_batches = 0
    for batch in loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        model.zero_grad(set_to_none=True)
        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = out.loss
        loss.backward()
        for n,p in model.named_parameters():
            if p.requires_grad and p.grad is not None:
                fisher[n] += (p.grad.detach() ** 2)
        n_batches += 1
        if n_batches >= max_batches: break
    for n in fisher:
        fisher[n] /= max(1, n_batches)
    model.zero_grad(set_to_none=True)
    return fisher

def ewc_loss(model: nn.Module, fisher: Dict[str, torch.Tensor], prev_params: Dict[str, torch.Tensor], lam: float):
    reg = 0.0
    for n,p in model.named_parameters():
        if p.requires_grad and (n in fisher):
            reg = reg + (fisher[n] * (p - prev_params[n])**2).sum()
    return lam * reg

def _sample_negatives_for_batch(
    input_ids: torch.Tensor, labels: torch.Tensor, item_token_ids: List[int], pad_id: int, cls_id: int, mask_id: int
) -> torch.Tensor:
    device = input_ids.device
    item_set = set(item_token_ids)
    specials = {pad_id, cls_id, mask_id}
    B, L = input_ids.shape
    neg = torch.empty(B, dtype=torch.long, device=device)
    for i in range(B):
        ctx = [int(x) for x in input_ids[i].tolist() if (x in item_set and x not in specials)]
        tgt = int(labels[i].item())
        ctx_set = set(ctx); ctx_set.add(tgt)
        while True:
            cand = int(item_token_ids[np.random.randint(0, len(item_token_ids))])
            if cand not in ctx_set:
                neg[i] = cand
                break
    return neg

def lwf_kd_loss_bert(
    student: nn.Module,
    teacher: nn.Module,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    labels: torch.Tensor,
    neg_ids: torch.Tensor,
    T: float = 2.0,
    alpha: float = 0.5,
):
    with torch.no_grad():
        t_out = teacher(input_ids=input_ids, attention_mask=attention_mask)
        t_logits = t_out.logits
        final_pos = attention_mask.sum(dim=1) - 1
        rows = torch.arange(input_ids.size(0), device=input_ids.device)
        t_final = t_logits[rows, final_pos, :] / T
        cand = torch.stack([labels, neg_ids], dim=1)      # [B, 2]
        t_cand = t_final.gather(1, cand)                  # [B, 2]
        t_probs = F.softmax(t_cand, dim=-1)

    s_out = student(input_ids=input_ids, attention_mask=attention_mask)
    s_logits = s_out.logits
    s_final = s_logits[rows, final_pos, :] / T
    s_cand = s_final.gather(1, cand)
    s_log_probs = F.log_softmax(s_cand, dim=-1)

    kd = F.kl_div(s_log_probs, t_probs, reduction='batchmean') * (T * T)
    return alpha * kd

# ------------------------------
# F-stage: LoRA finetuning (通用，供 mode 2~4 使用)
# ------------------------------
def finetune_one_epoch_lora(
    model: nn.Module,
    finetune_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler,
    device: torch.device,
    item_token_ids: List[int],
    pad_id: int, cls_id: int, mask_id: int,
    plugin: str = 'none',
    replay_buf: Optional[ReplayBuffer] = None,
    ewc_state: Optional[dict] = None,
    teacher: Optional[nn.Module] = None,
    lwf_T: float = 2.0,
    lwf_alpha: float = 0.5,
    grad_clip: float = 1.0,
    replay_ratio: float = 0.3,
    log_every: int = 100,
) -> float:
    model.train()
    total_loss = 0.0
    n_steps = 0

    for step, batch in enumerate(tqdm(finetune_loader, desc="Finetune(F)-LoRA", leave=False), start=1):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)  # [B]

        optimizer.zero_grad(set_to_none=True)

        out = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = out.logits  # [B, L, V]
        final_pos = attention_mask.sum(dim=1) - 1
        rows = torch.arange(input_ids.size(0), device=device)
        logits_final = logits[rows, final_pos, :]

        V = logits_final.size(-1)
        mask_vec = torch.full((V,), float('-inf'), device=device)
        mask_vec[item_token_ids] = 0.0
        logits_final = logits_final + mask_vec
        base_loss = F.cross_entropy(logits_final, labels)

        loss = base_loss
        replay_loss_val = torch.tensor(0.0, device=device)
        kd_loss_val = torch.tensor(0.0, device=device)
        ewc_loss_val = torch.tensor(0.0, device=device)

        if ('replay' in plugin) and (replay_buf is not None) and (replay_ratio > 0.0):
            rb = replay_buf.next_batch()
            r_input = rb['input_ids'].to(device)
            r_mask  = rb['attention_mask'].to(device)
            r_labels= rb['labels'].to(device)  # [B, L]
            r_out = model(input_ids=r_input, attention_mask=r_mask, labels=r_labels)
            replay_loss_val = r_out.loss
            loss = (1.0 - replay_ratio) * loss + replay_ratio * replay_loss_val

        if ('lwf' in plugin) and (teacher is not None):
            neg_ids = _sample_negatives_for_batch(input_ids, labels, item_token_ids, pad_id, cls_id, mask_id)
            kd_loss_val = lwf_kd_loss_bert(
                student=model, teacher=teacher,
                input_ids=input_ids, attention_mask=attention_mask,
                labels=labels, neg_ids=neg_ids,
                T=lwf_T, alpha=lwf_alpha,
            )
            loss = loss + kd_loss_val

        if ('ewc' in plugin) and (ewc_state is not None):
            ewc_loss_val = ewc_loss(model, ewc_state['fisher'], ewc_state['prev_params'], ewc_state['lam'])
            loss = loss + ewc_loss_val

        loss.backward()
        if grad_clip and grad_clip > 0:
            clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()
        if scheduler is not None:
            scheduler.step()

        total_loss += float(loss.detach().item())
        n_steps += 1

        if (log_every is not None) and (log_every > 0) and (step % log_every == 0):
            print(
                f"[F][step {step}] base:{base_loss.item():.4f} "
                f"replay:{replay_loss_val.item():.4f} kd:{kd_loss_val.item():.4f} ewc:{ewc_loss_val.item():.4f} "
                f"→ total:{loss.item():.4f}"
            )

    return total_loss / max(1, n_steps)

# =========================================================
# ===== RAIE 部分：保持你给出的核心实现与流程 =====
# =========================================================
def l2_normalize(X: np.ndarray, eps: float = 1e-12):
    n = np.linalg.norm(X, axis=1, keepdims=True) + eps
    return (X / n).astype(np.float32)

def spherical_kmeans(X: np.ndarray, K: int, niter=30, seed=42):
    try:
        import faiss
        km = faiss.Kmeans(d=X.shape[1], k=K, niter=niter, nredo=2,
                          verbose=True, spherical=True, seed=seed)
        km.train(X.astype(np.float32))
        C = km.centroids.astype(np.float32)
        index = faiss.IndexFlatIP(X.shape[1]); index.add(C)
        _, lab = index.search(X, 1)
        return C, lab.reshape(-1).astype(np.int32)
    except Exception:
        from sklearn.cluster import KMeans
        skm = KMeans(n_clusters=K, n_init=10, max_iter=300, random_state=seed)
        lab = skm.fit_predict(X)
        C = l2_normalize(skm.cluster_centers_)
        return C.astype(np.float32), lab.astype(np.int32)

def per_cluster_radius(X, C, labels, q=0.9):
    K = C.shape[0]; R = np.zeros(K, np.float32)
    for k in range(K):
        idx = np.where(labels == k)[0]
        if len(idx) == 0: continue
        dots = X[idx] @ C[k]
        ang = np.arccos(np.clip(dots, -1, 1))
        R[k] = np.quantile(ang, q).astype(np.float32)
    return R

def per_cluster_ang_std(X, C, labels):
    K = C.shape[0]; sig = np.zeros(K, np.float32)
    for k in range(K):
        idx = np.where(labels == k)[0]
        if len(idx) == 0: continue
        dots = X[idx] @ C[k]
        ang = np.arccos(np.clip(dots, -1, 1))
        sig[k] = ang.std().astype(np.float32)
    return sig

@torch.no_grad()
def batch_prompt_ids(batch_tokens: List[List[str]], token2id: Dict[str,int],
                     max_len: int, pad_id: int, cls_id: int):
    B = len(batch_tokens); L = max_len + 1
    input_ids = torch.full((B, L), pad_id, dtype=torch.long)
    attn = torch.zeros((B, L), dtype=torch.long)
    for i, toks in enumerate(batch_tokens):
        ids = [token2id.get(t, token2id['[UNK]']) for t in toks[-max_len:]]
        pos = 0; input_ids[i, pos] = cls_id; pos += 1
        if len(ids) > 0:
            m = min(len(ids), L - pos)
            input_ids[i, pos:pos+m] = torch.tensor(ids[-m:], dtype=torch.long)
            attn[i, :pos+m] = 1
        else:
            attn[i, :1] = 1  # only [CLS]
    return input_ids, attn

@torch.no_grad()
def encode_prompts_to_vecs(model, examples, token2id, max_len, pad_id, cls_id,
                           device, use_mean_pool=True, batch_size=256, pbar=False):
    vecs = []
    model.eval()
    if hasattr(model, "set_adapter"):
        try: model.set_adapter("default")
        except: pass

    for i in tqdm(range(0, len(examples), batch_size), disable=not pbar):
        batch = examples[i:i+batch_size]
        ptoks = []
        for ex in batch:
            if hasattr(ex, "prompt_tokens"): ptoks.append(ex.prompt_tokens)
            elif isinstance(ex, dict) and "prompt" in ex: ptoks.append(ex["prompt"].split())
            else: ptoks.append(ex.items if hasattr(ex, "items") else str(ex).split())

        ids, attn = batch_prompt_ids(ptoks, token2id, max_len, pad_id, cls_id)
        ids, attn = ids.to(device), attn.to(device)
        out = model.bert(input_ids=ids, attention_mask=attn, output_hidden_states=True, return_dict=True)
        h = out.last_hidden_state  # [B, L, H]
        if use_mean_pool:
            mask = attn.unsqueeze(-1).float()
            v = (h * mask).sum(dim=1) / (mask.sum(dim=1).clamp(min=1.0))
        else:
            v = h[:, 0, :]
        vecs.append(v.detach().float().cpu().numpy())
    X = np.concatenate(vecs, axis=0).astype(np.float32)
    return l2_normalize(X)

def finetune_one_epoch_bert(
    model: BertForMaskedLM,
    finetune_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler,
    device: torch.device,
    item_token_ids: List[int],
    grad_clip: float = 1.0,
    E0=None, ids_t=None, lambda_anchor=0.0
) -> float:
    model.train()
    total = 0.0
    n_steps = 0
    for batch in tqdm(finetune_loader, desc="Finetune(F)", leave=False):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad(set_to_none=True)

        out = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = out.logits  # [B, L, V]
        final_pos = attention_mask.sum(dim=1) - 1
        rows = torch.arange(input_ids.size(0), device=device)
        logits_final = logits[rows, final_pos, :]

        V = logits_final.size(-1)
        mask_vec = torch.full((V,), float('-inf'), device=device); mask_vec[item_token_ids] = 0.0
        logits_final = logits_final + mask_vec
        loss = F.cross_entropy(logits_final, labels)
        if lambda_anchor > 0.0 and (E0 is not None) and (ids_t is not None) and ids_t.numel()>0:
            E = model.get_input_embeddings().weight
            reg = (E.index_select(0, ids_t) - E0.index_select(0, ids_t)).pow(2).mean()
            loss = loss + lambda_anchor * reg

        loss.backward()
        if grad_clip and grad_clip > 0:
            clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()
        if scheduler is not None:
            scheduler.step()

        total += float(loss.detach().item())
        n_steps += 1

    return total / max(1, n_steps)

class RegionBank:
    def __init__(self, model, token2id, item_token_ids: List[int], original_ds, pad_id, cls_id,
                 max_len: int, device, out_dir: str,
                 K=10, q=0.9, T_low=0.7, T_high=0.9, tau=0.05, gamma=0.5, gap_thr=0.02,
                 lora_r=8, lora_alpha=16, lora_dropout=0.05, target_modules=("query","key","value","intermediate.dense","output.dense","attention.output.dense")):
        self.model = model
        self.token2id = token2id
        self.item_token_ids = item_token_ids
        self.pad_id, self.cls_id = pad_id, cls_id
        self.max_len = max_len
        self.device = device
        self.out_dir = out_dir
        self.original_ds = original_ds
        self.orig_idx_by_k = {}
        self.S = None
        self.n = None
        self.kappa = None
        self.pi = None
        self.C0 = None
        self.n0 = 5.0
        self.kappa_ema = 0.2
        self.beta_post = 8.0
        self.delta_min = 0.05
        self.lam_new = -1.0

        self.K = K; self.q = q
        self.T_low = T_low; self.T_high = T_high
        self.tau = tau; self.gamma = gamma; self.gap_thr = gap_thr

        self.lora_cfg = LoraConfig(
            r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
            bias="none", task_type=TaskType.FEATURE_EXTRACTION, inference_mode=False,
            target_modules=list(target_modules)
        )

        self.C = None
        self.R = None
        self.sig = None
        self.orig_idx_by_k = {}

    def fit_regions_on_original(self, original_ds, seed=42):
        X = encode_prompts_to_vecs(self.model, original_ds.examples, self.token2id,
                                   self.max_len, self.pad_id, self.cls_id, self.device, pbar=True)
        C, labels = spherical_kmeans(X, self.K, niter=30, seed=seed)
        R = per_cluster_radius(X, C, labels, q=self.q)
        sig = per_cluster_ang_std(X, C, labels)
        self.C, self.R, self.sig = C, R, sig
        self.C0 = C.copy()
        self._init_vmf_from_labels(X, labels)

        self.orig_idx_by_k = {k: [] for k in range(self.K)}
        for i, k in enumerate(labels):
            self.orig_idx_by_k[int(k)] = self.orig_idx_by_k.get(int(k), [])
            self.orig_idx_by_k[int(k)].append(i)

        np.savez(os.path.join(self.out_dir, "raie_regions_init.npz"),
                 centroids=C, radii=R, sig=sig, K=self.K, dim=C.shape[1])
        return labels

    def _sync_R_sig(self, new_centers=None, Xp=None, assign=None):
        K = self.C.shape[0]
        if self.R is None:  self.R = np.zeros((0,), np.float32)
        if self.sig is None: self.sig = np.zeros((0,), np.float32)

        if self.R.shape[0] < K:
            add_n = K - self.R.shape[0]
            mR = float(np.median(self.R)) if self.R.size else 0.30
            ms = float(np.median(self.sig)) if self.sig.size else 0.10
            addR = np.full((add_n,), mR, np.float32)
            addS = np.full((add_n,), ms, np.float32)
            if (new_centers is not None) and (Xp is not None) and (assign is not None):
                for t in range(min(add_n, len(new_centers))):
                    idx = np.where(assign == t)[0]
                    if idx.size:
                        mu = new_centers[t]
                        dots = Xp[idx] @ mu
                        ang = np.arccos(np.clip(dots, -1, 1))
                        addR[t] = np.quantile(ang, self.q).astype(np.float32)
                        addS[t] = ang.std().astype(np.float32)
            self.R = np.concatenate([self.R, addR])
            self.sig = np.concatenate([self.sig, addS])
        elif self.R.shape[0] > K:
            self.R = self.R[:K]
            self.sig = self.sig[:K]

    def _init_vmf_from_labels(self, X: np.ndarray, labels: np.ndarray):
        K, H = self.C.shape[0], X.shape[1]
        self.S = np.zeros((K, H), np.float32)
        self.n = np.zeros((K,), np.float32)
        for i, k in enumerate(labels.astype(int)):
            self.S[k] += X[i]
            self.n[k] += 1.0
        self._recompute_mu_kappa_pi()

    def _sync_priors_shape(self):
        K, H = self.S.shape
        if self.C0 is None:
            self.C0 = self.C.copy().astype(np.float32)
        if self.C0.shape[0] < K:
            S_ex = self.S[self.C0.shape[0]:K, :]
            norms = np.linalg.norm(S_ex, axis=1, keepdims=True)
            extra = np.zeros_like(S_ex, dtype=np.float32)
            ok = (norms.squeeze(-1) > 1e-12)
            if ok.any():
                extra[ok] = (S_ex[ok] / (norms[ok] + 1e-12)).astype(np.float32)
            if self.C is not None and self.C.shape[0] > 0:
                mean_mu = self.C.mean(0, keepdims=True)
                mean_mu = mean_mu / (np.linalg.norm(mean_mu, axis=1, keepdims=True) + 1e-12)
            else:
                mean_mu = np.zeros((1, H), np.float32); mean_mu[0, 0] = 1.0
            for r in range(extra.shape[0]):
                if not np.isfinite(extra[r]).all() or np.linalg.norm(extra[r]) < 1e-9:
                    extra[r] = mean_mu
            self.C0 = np.vstack([self.C0, extra.astype(np.float32)])
        elif self.C0.shape[0] > K:
            self.C0 = self.C0[:K, :]

    def _recompute_mu_kappa_pi(self):
        eps = 1e-12
        self._sync_priors_shape()

        S_hat = self.S + self.n0 * self.C0
        n_hat = self.n + self.n0

        normS = np.linalg.norm(S_hat, axis=1, keepdims=True) + eps
        C_new = (S_hat / normS).astype(np.float32)
        d = C_new.shape[1]

        r = (normS.squeeze(-1)) / np.maximum(n_hat, 1e-6)
        r = np.clip(r, 1e-6, 1 - 1e-6)
        kappa_new = (r * (d - r ** 2)) / np.clip(1.0 - r ** 2, 1e-6, None)
        kappa_new = np.clip(kappa_new, 1.0, 1e6).astype(np.float32)

        nsum = float(np.maximum(n_hat.sum(), 1.0))
        self.pi = (n_hat / nsum).astype(np.float32)

        if self.kappa is None:
            self.kappa = kappa_new
        else:
            K_old = int(self.kappa.shape[0])
            K_new = int(kappa_new.shape[0])
            if K_new == K_old:
                self.kappa = (1.0 - self.kappa_ema) * self.kappa + self.kappa_ema * kappa_new
            elif K_new > K_old:
                out = kappa_new.copy()
                out[:K_old] = (1.0 - self.kappa_ema) * self.kappa + self.kappa_ema * kappa_new[:K_old]
                self.kappa = out
            else:
                self.kappa = kappa_new

        self.C = C_new

    # def _scores_post(self, x: np.ndarray):
    #     sims = self.C @ x  # [K]
    #     scores = np.log(np.clip(self.pi, 1e-8, None)) + self.kappa * sims
    #     top2 = np.argsort(scores)[-2:]
    #     k2, k1 = int(top2[0]), int(top2[1])
    #     s1, s2 = float(scores[k1]), float(scores[k2])
    #     z = scores - scores.max()
    #     p = np.exp(self.beta_post * z); p = p / (p.sum() + 1e-8)
    #     return k1, k2, s1, s2, float(p[k1]), float(p[k2])
    def _scores_post(self, x: np.ndarray):
        sims = self.C @ x
        scores = np.log(np.clip(self.pi, 1e-8, None)) + self.kappa * sims

        # --- K=1: 不做 top-2，直接返回簇0 ---
        if scores.shape[0] == 1:
            z = scores - scores.max()
            p = np.exp(self.beta_post * z);
            p = p / (p.sum() + 1e-8)
            k1 = 0
            k2 = 0  # 占位
            s1 = float(scores[0])
            s2 = float('-inf')  # 占位
            p1 = float(p[0])
            p2 = 0.0  # 占位
            return k1, k2, s1, s2, p1, p2

        # --- K>=2 的原逻辑 ---
        top2 = np.argsort(scores)[-2:]
        k2, k1 = int(top2[0]), int(top2[1])
        s1, s2 = float(scores[k1]), float(scores[k2])
        z = scores - scores.max()
        p = np.exp(self.beta_post * z);
        p = p / (p.sum() + 1e-8)
        return k1, k2, s1, s2, float(p[k1]), float(p[k2])

    def _tau_assign(self, k: int, a0: float = 0.5, a1: float = 0.1):
        return 1.0 / (1.0 + np.exp(-(a0 + a1 * np.log(self.kappa[k] + 1e-6))))

    # def _decide_action(self, x: np.ndarray):
    #     k1, k2, s1, s2, p1, p2 = self._scores_post(x)
    #     tau1 = self._tau_assign(k1)
    #     if s1 < self.lam_new and p1 < tau1 * 0.7:
    #         return ("add", k1, k2, p1, p2)
    #     if p1 >= tau1 and (p1 - p2) >= self.delta_min:
    #         return ("update", k1, k2, p1, p2)
    #     if p1 >= tau1:
    #         return ("expand", k1, k2, p1, p2)
    #     return ("add", k1, k2, p1, p2)

    # def _decide_action(self, x: np.ndarray):
    #     # --- K=1: 不新增/不扩展，只更新簇0 ---
    #     if self.C.shape[0] == 1:
    #         return ("update", 0, 0, 1.0, 0.0)
    #
    #     k1, k2, s1, s2, p1, p2 = self._scores_post(x)
    #     tau1 = self._tau_assign(k1)
    #     if s1 < self.lam_new and p1 < tau1 * 0.7:
    #         return ("add", k1, k2, p1, p2)
    #     if p1 >= tau1 and (p1 - p2) >= self.delta_min:
    #         return ("update", k1, k2, p1, p2)
    #     if p1 >= tau1:
    #         return ("expand", k1, k2, p1, p2)
    #     return ("add", k1, k2, p1, p2)

    def _decide_action(self, x: np.ndarray):
        """
        """
        # K=1：直接更新簇0
        if self.C.shape[0] == 1:
            return ("update", 0, 0, 1.0, 0.0)

        # K>=2：找到后验分数最高的簇，仍返回 "update"
        k1, k2, s1, s2, p1, p2 = self._scores_post(x)
        return ("update", k1, k2, p1, p2)

    def _propose_new_clusters(self, X_pool: np.ndarray, max_new: int = 2):
        cand = []
        if X_pool.shape[0] == 0:
            return cand
        mu1 = X_pool.mean(0); mu1 /= (np.linalg.norm(mu1) + 1e-12)
        cand.append(mu1.astype(np.float32))
        if X_pool.shape[0] >= 50 and max_new >= 2:
            C2, lab2 = spherical_kmeans(X_pool, 2, niter=20, seed=42)
            cand.extend([C2[0], C2[1]])
        return cand[:max_new]

    def _accept_new_clusters(self, X_pool: np.ndarray, centers: List[np.ndarray],
                             bic_gain: float = 1e4) -> bool:
        if len(centers) == 0 or X_pool.shape[0] == 0:
            return False
        sims_old = X_pool @ self.C.T
        scores_old = np.log(np.clip(self.pi, 1e-8, None))[None, :] + sims_old * self.kappa[None, :]
        ll_old = float(np.max(scores_old, axis=1).sum())

        def est_kappa_from_assign(X, mu):
            dots = X @ mu
            rbar = float(np.mean(np.clip(dots, -1, 1)))
            d = X.shape[1]
            kappa = (rbar * (d - rbar**2)) / max(1e-6, (1 - rbar**2))
            return max(1.0, min(1e6, kappa))

        addC = np.stack(centers, 0).astype(np.float32)
        kappa_new = np.array([est_kappa_from_assign(X_pool, mu) for mu in addC], np.float32)
        sims_new = X_pool @ addC.T
        score_new = sims_new * kappa_new[None, :]
        w = np.maximum(score_new.mean(0), 1e-6)
        w = w / (w.sum() + 1e-8)
        eps = 0.10
        all_mu = np.vstack([self.C, addC])
        all_kappa = np.concatenate([self.kappa, kappa_new])
        all_pi = np.concatenate([self.pi * (1 - eps), w * eps])
        all_pi = all_pi / (all_pi.sum() + 1e-8)

        sims_all = X_pool @ all_mu.T
        scores_all = np.log(np.clip(all_pi, 1e-8, None))[None, :] + sims_all * all_kappa[None, :]
        ll_new = float(np.max(scores_all, axis=1).sum())

        d = X_pool.shape[1]
        K0 = self.C.shape[0]
        K1 = K0 + len(centers)
        bic_old = ll_old - 0.5 * (K0 * d) * np.log(X_pool.shape[0] + 1e-8)
        bic_new = ll_new - 0.5 * (K1 * d) * np.log(X_pool.shape[0] + 1e-8)

        return (bic_new - bic_old) > bic_gain

    def _maybe_merge_by_bic(self, angle_thr: float = np.deg2rad(12), max_pairs: int = 1):
        K = self.C.shape[0]
        if K <= 1:
            return
        pairs = []
        for i in range(K):
            for j in range(i + 1, K):
                ang = np.arccos(np.clip(self.C[i] @ self.C[j], -1, 1))
                if ang < angle_thr:
                    pairs.append((ang, i, j))
        pairs.sort()
        merged = 0
        for _, i, j in pairs:
            if i >= self.C.shape[0] or j >= self.C.shape[0]:
                continue
            Sij = self.S[i] + self.S[j]
            nij = self.n[i] + self.n[j]
            self.S[i] = Sij; self.n[i] = nij
            self.S = np.delete(self.S, j, axis=0)
            self.n = np.delete(self.n, j, axis=0)
            self.C = np.delete(self.C, j, axis=0)
            if self.R is not None:   self.R = np.delete(self.R, j, axis=0)
            if self.sig is not None: self.sig = np.delete(self.sig, j, axis=0)
            if self.kappa is not None: self.kappa = np.delete(self.kappa, j, axis=0)
            if self.pi is not None:    self.pi = np.delete(self.pi, j, axis=0)
            merged += 1
            if merged >= max_pairs:
                break
        self._recompute_mu_kappa_pi()
        self._sync_R_sig()

    # def map_finetune(self, finetune_ds, pool_min=200, bic_gain=1e4):
    #     Xn = encode_prompts_to_vecs(self.model, finetune_ds.examples, self.token2id,
    #                                 self.max_len, self.pad_id, self.cls_id, self.device, pbar=True)
    #     if self.C.shape[0] == 1:
    #         buckets = defaultdict(list)
    #         for i in range(Xn.shape[0]):
    #             buckets[0].append(i)
    #             self.S[0] += Xn[i];
    #             self.n[0] += 1.0
    #         self._recompute_mu_kappa_pi()
    #         np.savez(os.path.join(self.out_dir, "raie_regions_after_map.npz"),
    #                  centroids=self.C, radii=self.R, sig=self.sig,
    #                  kappa=self.kappa, pi=self.pi, S=self.S, n=self.n)
    #         return buckets, []
    #     buckets = defaultdict(list)
    #     add_pool = []
    #     soft_pairs = []
    #
    #     for i in range(Xn.shape[0]):
    #         x = Xn[i]
    #         act, k1, k2, p1, p2 = self._decide_action(x)
    #         if act == "update":
    #             buckets[k1].append(i)
    #             self.S[k1] += x; self.n[k1] += 1.0
    #         elif act == "expand":
    #             buckets[k1].append(i)
    #             s = p1 + p2 + 1e-8
    #             w1, w2 = p1 / s, p2 / s
    #             soft_pairs.append((i, k1, k2, float(w1), float(w2)))
    #             self.S[k1] += 0.25 * x; self.n[k1] += 0.25
    #         else:
    #             add_pool.append((i, k1))
    #             soft_pairs.append((i, k1, k2, 0.3, 0.0))
    #
    #     self._recompute_mu_kappa_pi()
    #
    #     if len(add_pool) >= pool_min:
    #         add_idx = [idx for (idx, _) in add_pool]
    #         Xp = Xn[add_idx]
    #         centers = self._propose_new_clusters(Xp, max_new=2)
    #         if self._accept_new_clusters(Xp, centers, bic_gain=bic_gain):
    #             for mu_new in centers:
    #                 self.S = np.vstack([self.S, mu_new[None, :]])
    #                 self.n = np.append(self.n, 1.0)
    #             self._recompute_mu_kappa_pi()
    #
    #             new_start = self.C.shape[0] - len(centers)
    #             sims = Xp @ np.stack(centers, 0).T
    #             assign = np.argmax(sims, axis=1)
    #             self._sync_R_sig(new_centers=centers, Xp=Xp, assign=assign)
    #             for j, idx in enumerate(add_idx):
    #                 buckets[new_start + int(assign[j])].append(idx)
    #             add_pool.clear()
    #         else:
    #             for idx, near_k1 in add_pool:
    #                 buckets[near_k1].append(idx)
    #             add_pool.clear()
    #     else:
    #         for idx, near_k1 in add_pool:
    #             buckets[near_k1].append(idx)
    #         add_pool.clear()
    #
    #     self._maybe_merge_by_bic(angle_thr=np.deg2rad(12), max_pairs=1)
    #
    #     np.savez(os.path.join(self.out_dir, "raie_regions_after_map.npz"),
    #              centroids=self.C, radii=self.R, sig=self.sig,
    #              kappa=self.kappa, pi=self.pi, S=self.S, n=self.n)
    #     return buckets, soft_pairs

    def map_finetune(self, finetune_ds, pool_min=200, bic_gain=1e4):
        """
        RAE-off：禁用区域级编辑（不扩展、不新增）。仅将 F 段样本映射到最相近的现有簇，
        并更新该簇的充分统计量 S/n，用于后续重估中心/浓度/先验权重。
        """
        Xn = encode_prompts_to_vecs(
            self.model, finetune_ds.examples, self.token2id,
            self.max_len, self.pad_id, self.cls_id, self.device, pbar=True
        )

        # K=1：所有样本进簇0
        if self.C.shape[0] == 1:
            buckets = defaultdict(list)
            for i in range(Xn.shape[0]):
                buckets[0].append(i)
                self.S[0] += Xn[i]
                self.n[0] += 1.0
            self._recompute_mu_kappa_pi()
            np.savez(os.path.join(self.out_dir, "raie_regions_after_map.npz"),
                     centroids=self.C, radii=self.R, sig=self.sig,
                     kappa=self.kappa, pi=self.pi, S=self.S, n=self.n)
            return buckets, []  # 不再返回 soft_pairs

        # K>=2：按后验分数最高的簇做“更新”，不再扩展/新增
        buckets = defaultdict(list)
        for i in range(Xn.shape[0]):
            x = Xn[i]
            # 仍复用后验打分，但无论阈值如何，一律分配到 k1
            k1, k2, s1, s2, p1, p2 = self._scores_post(x)
            buckets[int(k1)].append(i)
            self.S[int(k1)] += x
            self.n[int(k1)] += 1.0

        # 重估参数（中心/浓度/先验权重），半径/方差保持原估计，不做扩展/合并/新增
        self._recompute_mu_kappa_pi()
        np.savez(os.path.join(self.out_dir, "raie_regions_after_map.npz"),
                 centroids=self.C, radii=self.R, sig=self.sig,
                 kappa=self.kappa, pi=self.pi, S=self.S, n=self.n)

        # 不再提供 soft_pairs（无 expand）
        return buckets, []

    def _ensure_adapter(self, adapter_name: str):
        if adapter_name not in getattr(self.model, "peft_config", {}):
            default_dir = os.path.join(self.out_dir, "default")
            if os.path.isdir(default_dir):
                self.model.load_adapter(default_dir, adapter_name=adapter_name, is_trainable=True)
            else:
                self.model.add_adapter(adapter_name, self.lora_cfg)

    def train_regions(self, finetune_ds: Dataset, finetune_collator,
                      optimizer_ctor, scheduler_ctor,
                      epochs_per_region=2, batch_size=256, orig_mix_ratio=1, seed=42,
                      region_buckets: Dict[int, List[int]] = None,
                      soft_pairs: Optional[List[Tuple[int,int,int,float,float]]] = None,
                      soft_rep_factor: int = 3):
        assert region_buckets is not None
        soft_pairs = soft_pairs or []

        soft_idx_by_k = defaultdict(list)
        for (i, k1, k2, w1, w2) in soft_pairs:
            if w1 > 0:
                soft_idx_by_k[k1].extend([i] * max(0, int(round(w1 * soft_rep_factor))))
            if w2 > 0:
                soft_idx_by_k[k2].extend([i] * max(0, int(round(w2 * soft_rep_factor))))

        for k, idx_list in sorted(region_buckets.items(), key=lambda kv: -len(kv[1])):
            if len(idx_list) == 0:
                continue
            name = f"region_{k}"
            self._ensure_adapter(name)
            self.model.set_adapter(name)
            if hasattr(self.model, "train_adapter"):
                self.model.train_adapter(name)
            self.model.train()

            ft_subset = Subset(finetune_ds, idx_list)

            if (k in self.orig_idx_by_k) and (orig_mix_ratio > 0) and (self.original_ds is not None):
                orig_idx = self.orig_idx_by_k[k]
                n_ft = len(ft_subset)
                n_orig = int(round(n_ft * orig_mix_ratio / max(1e-8, 1.0 - orig_mix_ratio)))
                if n_orig > 0 and len(orig_idx) > 0:
                    rng = np.random.RandomState(seed)
                    sel = rng.choice(orig_idx, size=min(n_orig, len(orig_idx)), replace=False)
                    orig_subset = Subset(self.original_ds, sel)
                    base_ds = ConcatDataset([ft_subset, orig_subset])
                else:
                    base_ds = ft_subset
            else:
                base_ds = ft_subset

            if len(soft_idx_by_k[k]) > 0:
                soft_subset = Subset(finetune_ds, soft_idx_by_k[k])
                train_ds = ConcatDataset([base_ds, soft_subset])
            else:
                train_ds = base_ds

            dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0,
                            collate_fn=finetune_collator)
            optim = optimizer_ctor(self.model)
            sched = scheduler_ctor(optim, len(dl), epochs_per_region)

            for ep in range(epochs_per_region):
                pbar = tqdm(dl, desc=f"[RAIE] Region {k} Ep {ep+1}/{epochs_per_region}")
                for batch in pbar:
                    input_ids = batch['input_ids'].to(self.device)
                    attn = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    out = self.model(input_ids=input_ids, attention_mask=attn)
                    logits = out.logits
                    final_pos = attn.sum(dim=1) - 1
                    rows = torch.arange(input_ids.size(0), device=self.device)
                    logits_final = logits[rows, final_pos, :]

                    V = logits_final.size(-1)
                    mask = torch.full((V,), float('-inf'), device=self.device)
                    mask[self.item_token_ids] = 0.0
                    logits_final = logits_final + mask
                    loss = torch.nn.functional.cross_entropy(logits_final, labels)

                    optim.zero_grad(set_to_none=True)
                    loss.backward()
                    optim.step(); sched.step()
                    pbar.set_postfix({"loss": float(loss.detach().cpu())})

            save_dir = os.path.join(self.out_dir, f"adapter_region_{k}")
            os.makedirs(save_dir, exist_ok=True)
            self.model.save_pretrained(save_dir, selected_adapters=[name])

    @torch.no_grad()
    def route_and_eval(self, test_ds: Dataset, eval_collator, candidate_item_ids: List[int],
                       compute_metrics_fn, k_list=(5,10,20), batch_size=256):
        if hasattr(self.model, "set_adapter"):
            try:
                self.model.set_adapter("default")
            except:
                pass
        X = encode_prompts_to_vecs(self.model, test_ds.examples, self.token2id,
                                   self.max_len, self.pad_id, self.cls_id, self.device, pbar=True)
        sims = X @ self.C.T
        scores = np.log(np.clip(self.pi, 1e-8, None))[None, :] + sims * self.kappa[None, :]
        route_k = np.argmax(scores, axis=1)

        idx_by_k = defaultdict(list)
        for i, k in enumerate(route_k): idx_by_k[int(k)].append(i)

        all_stats = []
        for k in sorted(idx_by_k.keys()):
            name = f"region_{k}"
            adapter_dir = os.path.join(self.out_dir, f"adapter_region_{k}", f"{name}")
            if name not in getattr(self.model, "peft_config", {}):
                if os.path.isdir(adapter_dir):
                    self.model.load_adapter(adapter_dir, adapter_name=name, is_trainable=False)
                else:
                    default_dir = os.path.join(self.out_dir, "default")
                    if os.path.isdir(default_dir):
                        self.model.load_adapter(default_dir, adapter_name=name, is_trainable=False)
                    else:
                        self.model.add_adapter(name, self.lora_cfg)
            self.model.set_adapter(name)

            sub = Subset(test_ds, idx_by_k[k])
            dl = DataLoader(sub, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=eval_collator)
            stat = compute_metrics_fn(self.model, dl, self.device, candidate_item_ids, k_list)
            stat["cluster_id"] = int(k); stat["num_samples"] = len(sub)
            all_stats.append(stat)
        return all_stats

# ------- RAIE 评测辅助 -------
def _eval_fn_for_raie(peft_model, loader, device, item_token_ids, topk):
    peft_model.eval()
    sum_hit = {K: 0.0 for K in topk}; sum_dcg = {K: 0.0 for K in topk}; N = 0
    with torch.no_grad():
        for batch in loader:
            inp = batch['input_ids'].to(device); attn = batch['attention_mask'].to(device); lab = batch['labels'].to(device)
            out = peft_model(input_ids=inp, attention_mask=attn); logits = out.logits
            rows = torch.arange(inp.size(0), device=device); pos = attn.sum(1) - 1
            logits_final = logits[rows, pos, :]
            V = logits_final.size(-1); mask = torch.full((V,), float('-inf'), device=device)
            mask[item_token_ids] = 0.0; logits_final = logits_final + mask
            max_k = max(topk); _, topk_idx = torch.topk(logits_final, k=max_k, dim=1)
            labels_exp = lab.view(-1, 1).expand(-1, max_k); match = (topk_idx == labels_exp).float()
            for K in topk:
                mK = match[:, :K]; hit = mK.max(1).values; posK = torch.argmax(mK, 1)
                dcg = hit * (1.0 / torch.log2(posK.float() + 2.0))
                sum_hit[K] += float(hit.sum().item()); sum_dcg[K] += float(dcg.sum().item())
            N += inp.size(0)
    return {f"Recall@{K}": sum_hit[K] / max(1, N) for K in topk} | {f"NDCG@{K}": sum_dcg[K] / max(1, N) for K in topk}


def _adapter_final_logits(model, adapter_name: str, input_ids: torch.Tensor,
                          attention_mask: torch.Tensor, item_token_ids: List[int]):
    if hasattr(model, "set_adapter"):
        try:
            model.set_adapter(adapter_name)
        except Exception:
            pass
    out = model(input_ids=input_ids, attention_mask=attention_mask)
    logits = out.logits  # [B, L, V]
    final_pos = attention_mask.sum(dim=1) - 1
    rows = torch.arange(input_ids.size(0), device=input_ids.device)
    logits_final = logits[rows, final_pos, :]

    V = logits_final.size(-1)
    mask_vec = torch.full((V,), float('-inf'), device=input_ids.device)
    mask_vec[item_token_ids] = 0.0
    return logits_final + mask_vec


@torch.no_grad()
def evaluate_lsat(
    model: nn.Module,
    loader: DataLoader,
    device: torch.device,
    item_token_ids: List[int],
    long_adapter: str,
    short_adapter: str,
    alpha: float = 0.5,
    topk: Tuple[int, ...] = (5, 10, 20),
) -> Dict[str, float]:
    """Ensemble the long-term and short-term adapters following LSAT.

    alpha controls the contribution from the long-term adapter.
    """
    alpha = float(max(0.0, min(1.0, alpha)))
    sum_hit = {K: 0.0 for K in topk}
    sum_dcg = {K: 0.0 for K in topk}
    N = 0

    for batch in tqdm(loader, desc='Eval-LSAT', leave=False):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        logits_long = _adapter_final_logits(model, long_adapter, input_ids, attention_mask, item_token_ids)
        logits_short = _adapter_final_logits(model, short_adapter, input_ids, attention_mask, item_token_ids)
        logits_final = alpha * logits_long + (1.0 - alpha) * logits_short

        m = recall_ndcg_at_k(logits_final, labels, item_token_ids, topk)
        for K in topk:
            sum_hit[K] += float(m['hit'][K].item())
            sum_dcg[K] += float(m['dcg'][K].item())
        N += int(m['n'].item())

    return {f"Recall@{K}": sum_hit[K] / max(1, N) for K in topk} | {f"NDCG@{K}": sum_dcg[K] / max(1, N) for K in topk}

# =========================================================
# Main
# =========================================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type=str, default='lora_lwf',
                        choices=['base','lora','lora_replay','lora_lwf','lsat','raie'],
                        help='6种方案：base / lora / lora_replay / lora_lwf / lsat / raie')
    parser.add_argument('--data_dir', type=str, default='/home/zj/code/Amazon_electronics/',
                        help='包含 original_stream.jsonl, finetune.jsonl, test.jsonl(or original.jsonl), item_ids.json')
    parser.add_argument('--output_dir', type=str, default='./runs/Bert4rec_Amazon_electronics')

    # Sequence / vocab
    parser.add_argument('--max_len', type=int, default=20, help='Length of prompt (without [CLS])')

    # Model
    parser.add_argument('--hidden_size', type=int, default=256)
    parser.add_argument('--num_hidden_layers', type=int, default=4)
    parser.add_argument('--num_attention_heads', type=int, default=4)
    parser.add_argument('--intermediate_size', type=int, default=1024)
    parser.add_argument('--dropout', type=float, default=0.1)

    # Objective
    parser.add_argument('--objective', type=str, default='cloze', choices=['cloze', 'next'])
    parser.add_argument('--mask_prob', type=float, default=0.15)

    # O-stage Train
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--epochs', type=int, default=5)
    parser.add_argument('--lr', type=float, default=5e-4)
    parser.add_argument('--weight_decay', type=float, default=0.01)
    parser.add_argument('--warmup_ratio', type=float, default=0.1)
    parser.add_argument('--grad_clip', type=float, default=1.0)
    parser.add_argument('--seed', type=int, default=42)

    # F-stage Finetune (LoRA)
    parser.add_argument('--finetune_epochs', type=int, default=3)
    parser.add_argument('--finetune_batch_size', type=int, default=32)
    parser.add_argument('--finetune_lr', type=float, default=5e-4)
    parser.add_argument('--do_prompt_mask', action='store_true')
    parser.add_argument('--prompt_mask_prob', type=float, default=0.15)

    # Plugins
    parser.add_argument('--replay_ratio', type=float, default=0.3)
    parser.add_argument('--lwf_T', type=float, default=2.0)
    parser.add_argument('--lwf_alpha', type=float, default=0.5)
    parser.add_argument('--ewc_lambda', type=float, default=5.0)           # not used in 5 modes but preserved
    parser.add_argument('--ewc_max_batches', type=int, default=100)        # not used in 5 modes but preserved
    parser.add_argument('--lsat_alpha', type=float, default=0.6, help='Long-term adapter weight for LSAT logit fusion')
    parser.add_argument('--lsat_long_epochs', type=int, default=1, help='Epochs for long-term adapter refinement')
    parser.add_argument('--lsat_short_epochs', type=int, default=3, help='Epochs for short-term adapter tuning')

    # LoRA args
    parser.add_argument('--lora_r', type=int, default=8)
    parser.add_argument('--lora_alpha', type=int, default=16)
    parser.add_argument('--lora_dropout', type=float, default=0.05)
    parser.add_argument('--lora_target', type=str,
                        default='query, key, value, intermediate.dense, output.dense, attention.output.dense')

    # RAIE args（与你原实现一致且可调）
    parser.add_argument('--K', type=int, default=3)
    parser.add_argument('--q', type=float, default=0.9)
    parser.add_argument('--T_low', type=float, default=0.7)
    parser.add_argument('--T_high', type=float, default=0.9)
    parser.add_argument('--tau', type=float, default=0.05)
    parser.add_argument('--gamma', type=float, default=0.5)
    parser.add_argument('--gap_thr', type=float, default=0.02)
    parser.add_argument('--pool_min', type=int, default=120)
    parser.add_argument('--bic_gain', type=float, default=1e4)
    parser.add_argument('--orig_mix_ratio', type=float, default=0.3)
    parser.add_argument('--soft_rep_factor', type=int, default=2)

    parser.add_argument('--topk', type=str, default='5,10,20')

    args = parser.parse_args()

    # sanity
    if args.mode in ('lora','lora_replay','lora_lwf','lsat','raie') and (not PEFT_AVAILABLE):
        raise RuntimeError(f"peft not available: {_PEFT_ERR}")

    os.makedirs(args.output_dir, exist_ok=True)
    set_seed(args.seed)

    # Vocab
    token2id, id2token, item_token_ids = load_item_vocab(args.data_dir)
    pad_id = token2id['[PAD]']; cls_id = token2id['[CLS]']; mask_id = token2id['[MASK]']

    # Datasets
    train_path = os.path.join(args.data_dir, 'original_stream.jsonl')
    finetune_path = os.path.join(args.data_dir, 'finetune.jsonl')
    test_path_tgt = os.path.join(args.data_dir, 'original.jsonl')
    test_path_alt = os.path.join(args.data_dir, 'original.jsonl')  # fallback
    if not os.path.exists(train_path):
        raise FileNotFoundError('Missing original_stream.jsonl in data_dir')

    # test 优先 test.jsonl，否则回退 original.jsonl
    test_path = test_path_tgt if os.path.exists(test_path_tgt) else test_path_alt
    if not os.path.exists(test_path):
        raise FileNotFoundError('Missing test.jsonl or original.jsonl in data_dir')

    train_ds = StreamDataset(train_path)
    finetune_ds = NextItemDataset(finetune_path) if os.path.exists(finetune_path) else None
    test_ds = NextItemDataset(test_path)

    # RAIE 需要 original_stride1.jsonl（若缺失，用 original.jsonl 回退）
    original_stride_path = os.path.join(args.data_dir, 'original_stride1.jsonl')
    original_for_raie_path = original_stride_path if os.path.exists(original_stride_path) else test_path_alt
    original_ds_for_raie = NextItemDataset(original_for_raie_path) if (args.mode=='raie') else None

    # Collators & loaders
    if args.objective == 'cloze':
        train_collator = ClozeTrainCollator(token2id, args.max_len, pad_id, mask_id, cls_id, args.mask_prob)
    else:
        # 若选择 next 目标，你可以自定义一个 next 训练 collator；为保持逻辑不改动，这里仍使用 Cloze 训练 O-stage
        train_collator = ClozeTrainCollator(token2id, args.max_len, pad_id, mask_id, cls_id, args.mask_prob)

    FT_collator = EvalCollator(token2id, args.max_len, mask_id, cls_id, pad_id,
                               do_prompt_mask=args.do_prompt_mask, prompt_mask_prob=args.prompt_mask_prob)
    eval_collator = EvalCollator(token2id, args.max_len, mask_id, cls_id, pad_id)

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=2,
                              collate_fn=train_collator)
    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=2,
                             collate_fn=eval_collator)

    # Model config
    vocab_size = len(id2token)
    max_position_embeddings = args.max_len + 2
    config = BertConfig(
        vocab_size=vocab_size,
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        num_attention_heads=args.num_attention_heads,
        intermediate_size=args.intermediate_size,
        max_position_embeddings=max_position_embeddings,
        hidden_dropout_prob=args.dropout,
        attention_probs_dropout_prob=args.dropout,
        pad_token_id=pad_id,
    )
    device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')
    base_model = BertForMaskedLM(config).to(device)

    # O-stage optimizer & scheduler
    no_decay = ['bias', 'LayerNorm.weight']
    params = [
        {
            'params': [p for n, p in base_model.named_parameters() if not any(nd in n for nd in no_decay)],
            'weight_decay': args.weight_decay,
        },
        {
            'params': [p for n, p in base_model.named_parameters() if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0,
        },
    ]
    optimizer = torch.optim.AdamW(params, lr=args.lr)
    total_steps = len(train_loader) * max(args.epochs, 1)
    warmup_steps = int(total_steps * args.warmup_ratio)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,
                                                num_training_steps=total_steps)

    # ---- Stage O: train on original_stream.jsonl ----
    for epoch in range(1, args.epochs + 1):
        tr = train_one_epoch(base_model, train_loader, optimizer, scheduler, device,
                             item_token_ids, objective=args.objective, grad_clip=args.grad_clip)
        print(f"[O][Epoch {epoch}] train_loss = {tr['train_loss']:.4f}")

    # always save base
    base_dir = os.path.join(args.output_dir, 'base_model')
    os.makedirs(base_dir, exist_ok=True)
    base_model.save_pretrained(base_dir)
    with open(os.path.join(base_dir, 'vocab.json'), 'w', encoding='utf-8') as f:
        json.dump({'id2token': id2token}, f, ensure_ascii=False, indent=2)

    # =============================
    # Modes
    # =============================
    topk_tuple = tuple(int(x) for x in args.topk.split(','))

    # ---------- 方案1：base ----------
    if args.mode == 'base':
        print("[Mode] base: O-stage only, no LoRA / plugins")
        metrics = evaluate(base_model, test_loader, device, item_token_ids, topk=topk_tuple)
        print("[Test][base]", ' '.join([f"{k}:{v:.4f}" for k, v in sorted(metrics.items())]))
        with open(os.path.join(args.output_dir, 'test_metrics_base.json'), 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        return

    # ---------- 公共：LoRA 包装（2~4 & RAIE 均需） ----------
    target_modules = [s.strip() for s in args.lora_target.split(',') if s.strip()]
    lora_cfg = LoraConfig(
        r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
        bias='none', task_type=TaskType.FEATURE_EXTRACTION, target_modules=target_modules
    )
    lora_model = get_peft_model(base_model, lora_cfg)
    try:
        lora_model.print_trainable_parameters()
    except:
        pass

    # ---------- 方案2/3/4/新对比：LoRA 微调 ----------
    if args.mode in ('lora','lora_replay','lora_lwf','lsat'):
        if (finetune_ds is None) or (len(finetune_ds) == 0):
            if args.mode == 'lsat':
                raise RuntimeError('[LSAT] finetune.jsonl is required.')
            print("[Warn] finetune.jsonl not found or empty; skip F-stage, evaluate O-stage+LoRA(=base) directly.")
            eval_model = base_model
        else:
            FT_loader = DataLoader(finetune_ds, batch_size=args.finetune_batch_size, shuffle=True, num_workers=2,
                                   collate_fn=FT_collator)

            if args.mode == 'lsat':
                long_adapter = 'default'
                short_adapter = 'short_term'
                if short_adapter not in getattr(lora_model, 'peft_config', {}):
                    lora_model.add_adapter(short_adapter, lora_cfg)

                if hasattr(lora_model, "set_adapter"):
                    try:
                        lora_model.set_adapter(long_adapter)
                        if hasattr(lora_model, "train_adapter"):
                            lora_model.train_adapter(long_adapter)
                    except Exception:
                        pass

                long_params = [p for p in lora_model.parameters() if p.requires_grad]
                long_optim = torch.optim.AdamW(long_params, lr=args.finetune_lr)
                long_steps = len(train_loader) * max(args.lsat_long_epochs, 1)
                long_warm = int(long_steps * args.warmup_ratio)
                long_sched = get_linear_schedule_with_warmup(long_optim, num_warmup_steps=long_warm, num_training_steps=long_steps)
                for ep in range(1, args.lsat_long_epochs + 1):
                    tr = train_one_epoch(
                        lora_model, train_loader, long_optim, long_sched, device, item_token_ids,
                        objective=args.objective, grad_clip=args.grad_clip
                    )
                    print(f"[LSAT][Long][Epoch {ep}] loss={tr['train_loss']:.4f}")

                if hasattr(lora_model, "set_adapter"):
                    try:
                        lora_model.set_adapter(short_adapter)
                        if hasattr(lora_model, "train_adapter"):
                            lora_model.train_adapter(short_adapter)
                    except Exception:
                        pass

                ft_params = [p for p in lora_model.parameters() if p.requires_grad]
                ft_optim = torch.optim.AdamW(ft_params, lr=args.finetune_lr)
                ft_steps = len(FT_loader) * max(args.lsat_short_epochs, 1)
                ft_warm = int(ft_steps * args.warmup_ratio)
                ft_sched = get_linear_schedule_with_warmup(ft_optim, num_warmup_steps=ft_warm, num_training_steps=ft_steps)

                for ep in range(1, args.lsat_short_epochs + 1):
                    ft_loss = finetune_one_epoch_lora(
                        lora_model, FT_loader, ft_optim, ft_sched, device, item_token_ids,
                        pad_id, cls_id, mask_id,
                        plugin='none', replay_buf=None, ewc_state=None,
                        teacher=None, lwf_T=args.lwf_T, lwf_alpha=args.lwf_alpha,
                        grad_clip=args.grad_clip, replay_ratio=args.replay_ratio, log_every=100
                    )
                    print(f"[LSAT][Short][Epoch {ep}] loss={ft_loss:.4f}")

                metrics = evaluate_lsat(
                    lora_model, test_loader, device, item_token_ids,
                    long_adapter=long_adapter, short_adapter=short_adapter,
                    alpha=args.lsat_alpha, topk=topk_tuple
                )
                tag = args.mode
                print(f"[Test][{tag}]", ' '.join([f"{k}:{v:.4f}" for k, v in sorted(metrics.items())]))
                with open(os.path.join(args.output_dir, f'test_metrics_{tag}.json'), 'w', encoding='utf-8') as f:
                    json.dump(metrics, f, ensure_ascii=False, indent=2)
                long_dir = os.path.join(args.output_dir, 'lora_adapter_lsat_long')
                short_dir = os.path.join(args.output_dir, 'lora_adapter_lsat_short')
                os.makedirs(long_dir, exist_ok=True); os.makedirs(short_dir, exist_ok=True)
                lora_model.save_pretrained(long_dir, selected_adapters=[long_adapter])
                lora_model.save_pretrained(short_dir, selected_adapters=[short_adapter])
                return

            ft_params = [p for p in lora_model.parameters() if p.requires_grad]
            ft_optim = torch.optim.AdamW(ft_params, lr=args.finetune_lr)
            ft_steps = len(FT_loader) * max(args.finetune_epochs, 1)
            ft_warm = int(ft_steps * args.warmup_ratio)
            ft_sched = get_linear_schedule_with_warmup(ft_optim, num_warmup_steps=ft_warm, num_training_steps=ft_steps)

            # plugins
            replay_buf = ReplayBuffer(train_ds, train_collator, batch_size=args.finetune_batch_size) \
                         if (args.mode == 'lora_replay') else None
            teacher = None
            if args.mode == 'lora_lwf':
                teacher = BertForMaskedLM.from_pretrained(base_dir).to(device)
                for p in teacher.parameters():
                    p.requires_grad = False
                teacher.eval()

            plugin_flag = 'none'
            if args.mode == 'lora_replay': plugin_flag = 'replay'
            if args.mode == 'lora_lwf':    plugin_flag = 'lwf'

            for ep in range(1, args.finetune_epochs + 1):
                ft_loss = finetune_one_epoch_lora(
                    lora_model, FT_loader, ft_optim, ft_sched, device, item_token_ids,
                    pad_id, cls_id, mask_id,
                    plugin=plugin_flag,
                    replay_buf=replay_buf,
                    ewc_state=None,  # 保留接口但不启用
                    teacher=teacher,
                    lwf_T=args.lwf_T, lwf_alpha=args.lwf_alpha,
                    grad_clip=args.grad_clip, replay_ratio=args.replay_ratio, log_every=100
                )
                print(f"[F][Epoch {ep}] LoRA finetune_loss = {ft_loss:.4f}")

            # eval with LoRA
            eval_model = lora_model

        metrics = evaluate(eval_model, test_loader, device, item_token_ids, topk=topk_tuple)
        tag = args.mode
        print(f"[Test][{tag}]", ' '.join([f"{k}:{v:.4f}" for k, v in sorted(metrics.items())]))
        with open(os.path.join(args.output_dir, f'test_metrics_{tag}.json'), 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        # 可选保存 LoRA
        lora_dir = os.path.join(args.output_dir, f'lora_adapter_{tag}')
        os.makedirs(lora_dir, exist_ok=True)
        lora_model.save_pretrained(lora_dir)
        return

    # ---------- 方案5：RAIE ----------
    assert args.mode == 'raie', "Internal guard"
    if (finetune_ds is None) or (len(finetune_ds) == 0):
        raise RuntimeError("[RAIE] finetune.jsonl is required.")

    # F 段开始前：保存基线嵌入快照 + 仅对 F 段目标 item 做选择性 embedding 微调（与你原实现一致）
    emb_in = lora_model.get_input_embeddings()
    E0 = emb_in.weight.detach().clone()
    tune_ids = sorted({token2id.get(ex.target_token, None)
                       for ex in finetune_ds.examples
                       if token2id.get(ex.target_token, None) in item_token_ids})
    ids_t = torch.tensor(tune_ids, device=emb_in.weight.device) if len(tune_ids) > 0 else None
    lambda_anchor = 1e-4

    # register grad mask ONLY for RAIE branch
    emb_in.weight.requires_grad_(True)
    mask = torch.zeros_like(emb_in.weight, dtype=torch.bool)
    if len(tune_ids) > 0:
        ids = torch.tensor(sorted(tune_ids), device=emb_in.weight.device, dtype=torch.long)
        mask[ids] = True
    def grad_mask(g):
        return g.masked_fill(~mask, 0)
    emb_in.weight.register_hook(grad_mask)
    emb_out = lora_model.get_output_embeddings()
    if emb_out is not None and emb_out.weight is not emb_in.weight:
        emb_out.weight.requires_grad_(True)
        emb_out.weight.register_hook(grad_mask)

    # F-stage for RAIE: 使用 finetune_one_epoch_bert（与你 RAIE 代码一致）
    FT_loader = DataLoader(finetune_ds, batch_size=args.finetune_batch_size, shuffle=True, num_workers=0,
                           collate_fn=FT_collator)
    ft_params = [p for p in lora_model.parameters() if p.requires_grad]
    ft_optim = torch.optim.AdamW(ft_params, lr=args.finetune_lr)
    ft_steps = len(FT_loader) * max(args.finetune_epochs, 1)
    ft_warm = int(ft_steps * args.warmup_ratio)
    ft_sched = get_linear_schedule_with_warmup(ft_optim, num_warmup_steps=ft_warm, num_training_steps=ft_steps)

    for ep in range(args.finetune_epochs):
        ft_loss = finetune_one_epoch_bert(
            lora_model, FT_loader, ft_optim, ft_sched, device, item_token_ids,
            grad_clip=args.grad_clip, E0=E0, ids_t=ids_t, lambda_anchor=lambda_anchor
        )
        print(f"[RAIE][F][Epoch {ep+1}] loss={ft_loss:.4f}")

    emb_in.weight.requires_grad_(False)
    if emb_out is not None and emb_out.weight is not emb_in.weight:
        emb_out.weight.requires_grad_(False)

    # 保存 default 适配器（供 RegionBank 默认加载）
    default_dir = os.path.join(args.output_dir, "default")
    os.makedirs(default_dir, exist_ok=True)
    lora_model.save_pretrained(default_dir, selected_adapters=["default"])

    # 构建 RegionBank 并完成 RAIE 流程
    bank = RegionBank(
        model=lora_model,
        token2id=token2id,
        item_token_ids=item_token_ids,
        pad_id=pad_id, cls_id=cls_id,
        max_len=args.max_len,
        device=device,
        out_dir=args.output_dir,
        K=args.K, q=args.q, T_low=args.T_low, T_high=args.T_high, tau=args.tau, gamma=args.gamma, gap_thr=args.gap_thr,
        lora_r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
        target_modules=tuple(target_modules),
        original_ds=original_ds_for_raie
    )
    if original_ds_for_raie is None:
        raise FileNotFoundError("[RAIE] Need original_stride1.jsonl or original.jsonl for region fitting.")
    _ = bank.fit_regions_on_original(original_ds_for_raie)
    region_buckets, soft_pairs = bank.map_finetune(finetune_ds, pool_min=args.pool_min, bic_gain=args.bic_gain)

    def _optim_ctor(m):
        no_decay = ['bias', 'LayerNorm.weight']
        groups = [
            {'params': [p for n, p in m.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)],
             'weight_decay': args.weight_decay},
            {'params': [p for n, p in m.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],
             'weight_decay': 0.0}
        ]
        return torch.optim.AdamW(groups, lr=args.finetune_lr)

    def _sched_ctor(optim, steps_per_epoch, epochs):
        total = steps_per_epoch * epochs
        warm = int(total * args.warmup_ratio)
        return get_linear_schedule_with_warmup(optim, num_warmup_steps=warm, num_training_steps=total)

    bank.train_regions(
        finetune_ds=finetune_ds,
        finetune_collator=FT_collator,
        optimizer_ctor=_optim_ctor,
        scheduler_ctor=_sched_ctor,
        epochs_per_region=args.finetune_epochs,
        batch_size=args.finetune_batch_size,
        orig_mix_ratio=args.orig_mix_ratio,
        region_buckets=region_buckets,
        soft_pairs=soft_pairs,
        soft_rep_factor=args.soft_rep_factor
    )

    stats = bank.route_and_eval(test_ds, eval_collator, item_token_ids, _eval_fn_for_raie,
                                k_list=topk_tuple)
    import pandas as pd
    df = pd.DataFrame(stats)
    w = df["num_samples"]; W = max(1, w.sum())
    global_metrics = {m: float((df[m] * w).sum() / W) for m in df.columns if m.startswith(("Recall@", "NDCG@"))}
    print("[RAIE][Global]", global_metrics)

    with open(os.path.join(args.output_dir, "raie_test_global.json"), "w", encoding="utf-8") as f:
        json.dump(global_metrics, f, ensure_ascii=False, indent=2)
    df.to_json(os.path.join(args.output_dir, "raie_cluster_stats.json"), orient="records", force_ascii=False, indent=2)

if __name__ == '__main__':
    main()
